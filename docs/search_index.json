[
["index.html", "Introduction To Basic Statistics in R 1 Introduction", " Introduction To Basic Statistics in R Version 1.1.0 Last update: November 22nd 2019 1 Introduction This document serves as a simple introduction to some of the basic principles in statistics in R. "],
["firstlook.html", "2 First look at R", " 2 First look at R R is in many ways similar to SPSS, and in a lot more ways far superior. Many like the point-and-click interface from SPSS, which R does not have, but I believe the coding skills necessary to use R are not complicated for most purposes and I’ve never met someone who started learning it and then went back to SPSS after. Here I’ll just briefly mention the very basics. I’ll assume you’re working within Rstudio. Just for clarification, R is the programming language, Rstudio is a software that facilitates coding in R. Rstudio has a panel dedicated to writing a script, a panel dedicated to the “console” on the bottom, this is where you can run the code, and it’s where R gives you feedback. The right side of Rstudio contains a panel for the “environment”, which is where your variables will go. The bottom right panel is dedicated for plots, and the help menu. In R you crate variables by assigning a value to a variable. For instance a &lt;- 1 assigns the number 1 to the variable called a. Then you can use this variable later in for instance something basic as aritmatic. The print() function does nothing more than print the variable to the console. a &lt;- 1 b &lt;- a + 2 print(b) ## [1] 3 The most-used variable type in R is the data frame. It resembles an SPSS dataset the most. It is a variable with multiple rows for observations, and columns for variables. Column names in a data frame are for instance ID, age, sex and so on. Let’s create a simple data frame. We do this with the data.frame() function, where we store variables with the c() function. Usually you would load data into R via read_csv(), read.table(), or by manually loading it in via the interactive interface. We’ll create a data frame with some names and some scores corresponding with each name. data &lt;- data.frame( names = c(&quot;Lucas&quot;, &quot;Linn&quot;, &quot;Thomas&quot;, &quot;Sara&quot;, &quot;Anna&quot;), scores = c(7.5, 8, 2, 6.5, 9) ) print(data) ## names scores ## 1 Lucas 7.5 ## 2 Linn 8.0 ## 3 Thomas 2.0 ## 4 Sara 6.5 ## 5 Anna 9.0 There are a few different variable types, similar to SPSS, there’s the numeric or integer variable type, for continuous data, the character string for text etc., the logical or binary type (TRUE or FALSE), and the factor, which is for categorical data. Strings are typically treated as categorical data. R is pretty smart when it comes to these variable types, so it’s quite rare you have to manually set the variable type. Strings are used in R in double quotes, e.g. \"Ole\" is a string, but a is the variable name. Another important aspect of R are the packages. Packages are collections of scripts that serve a particular purpose. A lot of funtions we’ll use here are built into R directly, but some will come from different packages. Popular packages in the field of neuroscience are {tidyverse}, {cowplot}, {psych}, and {lme4}. A package dedicated for NORMENT specific functions exists too, called the {normentR} package. You install packages via the install.packages() function. Since the {normentR} package is not part of the default packages in R, you’re going to have to ask me to install it for you for now. Let’s install the {tidyverse} package and the {psych} package: install.packages(&quot;tidyverse&quot;) install.packages(&quot;psych&quot;) This will take a few seconds and provide a bunch of messages in the console, you can ignore this. The install.packages() function downloads the package from a central repository, called CRAN. If you ever encounter a package on GitHub, another large hub for R packages, you need to install this via the {devtools} package. Then from this package, you use the install_github() function, with the GitHub repository of the package. Since it’s rare that you’d ever want to use the {devtool} package outside of this context, I prefer not to load it, but instead use a little trick which makes it possible to use the function within this package anyway without loading it, shown below. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;norment/normentR&quot;) The :: tells R that you want to use the install_github() from the {devtools} package without having to load the entire package. Then when the packages are installed, you load them via the library() function. Be aware that during the installation and the loading of packages, you might get some messages, and some might in red and look scary. Before you call the support team, I think you should know that these message are usually normal and harmless, and unless the messages say something like package &lt;packagename&gt; is not available (for R version 3.&lt;version&gt;.&lt;release&gt;) or Error in library(&lt;packagename&gt;) : there is no package called &lt;packagename&gt;, it requires little to no further action from you. If it does require action, it will usually explicity ask you. library(tidyverse) Now all functions within the {tidyverse} package is ready for you to use! If you’re unsure about the function and usage of a function, you can use the help function. This is very simple, you type a question mark, directly followed by the function name you want to see help from, for instance ?mean will show the documentation for the mean() function. "],
["definitions-of-basic-princples.html", "3 Definitions of basic princples", " 3 Definitions of basic princples So the most common statistic used anywhere is the mean. The mean is equivalent to the average in common terms. Let’s say you’re a primary school teacher with only 5 students who took a history exam. Now you’re interested in what the average score across these 5 students was. The scores from this test are stored in a data frame called data we created earlier. print(data) ## names scores ## 1 Lucas 7.5 ## 2 Linn 8.0 ## 3 Thomas 2.0 ## 4 Sara 6.5 ## 5 Anna 9.0 So what we do is we calculate the average (or mean) of the scores. We do this by running the mean() function across the scores column. mean(data$scores) ## [1] 6.6 We find that the mean score for this test is 6.6, which is a bit low. How come? Well, looking at the data we can see that Thomas didn’t do very well on this test, he scored a 2, which brought down the average of group considerable. We call Thomas’ score an outlier, because it lies outside the group of scores of his fellow students. A more robust measure in this case is the median. Where the mean sums all scores and then divides by the number of students, the median takes the scores, sorts them, and then takes the middle value. The R function that calculates the median is appropriately called median(). median(data$scores) ## [1] 7.5 In this case, that score is 7.5. The principle of the median is that there is an equal number of observations higher than the median, and an equal number of observations lower. The median is more robust to outliers, and will give a better indication of the performance of the majority of the group in this case. Another function that combines the previous functions and a few more, is the summary() function, which is very flexible and can be used in many different situations. If you for instance type summary(data$scores), it will give you the mean, median, minimum and maximum value, and the quantiles of the list of scores. Here’s what that looks like: summary(data$scores) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.0 6.5 7.5 6.6 8.0 9.0 The third descriptive is called the mode. The mode represents the value most observed. The mode is hardly ever used, because of its terrible job at accurately describing the data that the mean or median don’t do better. It’s so rare that there isn’t even a built-in function to calculate the mode directly. In this example, since no two students got the same score, it is particularly useless anyway. You may forget about he mode. "],
["uncertainty.html", "4 Uncertainty", " 4 Uncertainty Mean values are often reported directly together with the standard deviation. The standard deviation denotes the amount of variation of a set of values. If a set of values lie close together, the certainty that the mean is accurate is quite high, since there’s little variation. If there’s a larger variation in the values, the standard deviation is larger. Standard devation can be calculated in R via the function sd(). sd(data$scores) ## [1] 2.724885 The standard devation of the scores is about 2.7. Let’s say that, for the purpose of this experiment, you ask a group of high-school students to take the same test. Not suprisingly, they all did quite well. print(data_highschool) ## names scores ## 1 Alex 8.5 ## 2 Solveig 9.0 ## 3 Ivar 8.5 ## 4 Yasmin 9.5 ## 5 Tobbe 8.0 The standard deviation of the second dataset is less than 0.6. This reflects that the variation in the second dataset is much lower, since the scores are closer together. A low standard deviation by definition means that the range of values lie close to the mean. The standard deviation says something about the values and the dataset. A way to measure the (un)certainty of a statistic, such as the mean, is the standard error (fully called the “standard error of the mean”). The standard error makes claims about the variance in the statistic, in this case the mean. In simplified terms, the standard error is the answer to the question “how accurate is this mean?”. Since it transcends the measured data, the standard error estimates how close the mean of your measured data reflects the mean of the whole sample. Let’s say that these 5 students are selected from a larger group of 20 students, then the standard error of the mean of these 5 exams, indicates how uncertain you are that then you measure all 20 students, you would obtain the same mean. While separate, the standard deviation and the standard error are linked. For instance, you can calculate the standard error by just using the standard deviation and the number of samples. The formula for is the standard deviation divided by the square root of the number of observations. In R code this will be sd(x)/sqrt(length(x)) where x is the column with observations. The length() of x in this case will yield the number 5, since that’s the number of observations we have in this dataset. With this formula, we can calculate the standard error for the test scores from our primary school students. sd(data$scores)/sqrt(length(data$scores)) ## [1] 1.218606 Since we have only 5 students, the standard error is quite large, since we can’t be very certain that the mean of a larger group of students will be the same. "],
["distributions-probabilities.html", "5 Distributions &amp; Probabilities", " 5 Distributions &amp; Probabilities Distributions are very important in statistics. The simplest distrubition is the uniform distribution. For example, let’s take an experiment where we roll one dice a number of time. If you roll it properly, the chance of getting any one side of the dice to face up is exactly 1/6th. Then let’s say you roll that dice 900 times. In a truly random universe, after 900 tosses, each side will have faced up exactly 150 times. We can simulate this with the sample() function. sides &lt;- 1:6 tosses &lt;- 900 outcomes &lt;- sample(sides, tosses, replace = TRUE) # replace = TRUE is necessary to # tell the function that we accept that the same outcome may occur multiple times print(outcomes) ## [1] 2 6 3 1 2 6 4 6 6 4 6 2 3 2 5 4 3 3 2 1 2 3 4 4 6 4 5 3 1 3 4 2 6 5 5 6 6 5 1 5 4 3 6 1 1 1 ## [47] 6 3 1 1 6 4 6 3 5 6 3 6 4 3 3 3 6 4 3 6 6 5 4 4 3 1 6 2 5 3 5 4 5 4 4 5 1 2 1 2 4 5 2 2 4 1 ## [93] 2 3 3 3 2 2 1 6 5 6 3 2 3 2 3 4 2 5 6 1 4 6 6 2 3 6 2 6 3 3 4 1 2 6 2 4 4 5 4 5 2 4 4 1 2 3 ## [139] 6 5 1 4 1 2 2 5 6 2 1 2 1 4 2 3 6 2 1 2 1 5 1 5 6 3 4 2 6 2 2 4 4 2 5 2 6 6 1 5 4 3 5 6 4 6 ## [185] 5 4 6 1 5 4 3 3 3 4 4 2 6 2 4 5 1 5 4 2 5 4 4 3 6 4 4 4 2 1 4 2 6 3 3 5 3 6 2 6 2 6 3 3 4 2 ## [231] 4 4 1 3 1 3 2 4 1 6 3 3 1 2 1 2 5 2 6 4 4 1 1 5 3 1 3 4 5 5 5 5 2 5 6 6 5 5 2 3 5 5 2 1 1 3 ## [277] 4 4 3 5 4 3 2 1 2 4 1 3 5 3 1 1 5 2 6 4 1 2 5 2 5 3 1 4 1 2 6 5 6 3 5 3 2 2 5 6 1 5 5 3 2 1 ## [323] 2 5 1 3 2 5 6 5 5 5 4 1 4 4 1 6 1 4 6 5 2 5 2 5 5 5 1 1 4 3 2 2 3 1 1 3 3 4 2 2 5 6 6 2 4 2 ## [369] 6 5 2 4 6 4 2 1 3 2 5 3 5 6 4 5 1 4 4 5 3 5 4 3 6 2 4 2 2 4 3 1 2 1 1 2 3 6 2 3 4 3 1 5 5 1 ## [415] 1 4 3 1 1 5 2 3 1 4 1 6 5 6 5 6 1 5 2 4 3 5 2 5 2 5 5 6 2 2 2 2 2 3 1 2 5 4 2 6 1 6 4 4 1 5 ## [461] 5 6 6 5 2 5 3 1 4 1 4 5 6 1 1 1 3 1 4 4 1 2 5 6 1 4 3 4 1 5 2 2 6 3 1 4 4 4 3 5 5 4 5 4 6 5 ## [507] 3 2 1 6 6 4 5 1 4 1 6 2 5 6 6 4 3 1 5 6 4 6 6 1 1 4 2 1 2 6 4 5 6 5 2 1 6 6 4 3 2 2 3 6 5 4 ## [553] 1 3 3 1 1 4 6 6 3 4 6 2 2 6 5 5 1 3 1 3 2 4 5 3 5 6 3 6 3 3 3 2 1 6 6 1 2 4 5 2 6 6 2 2 1 2 ## [599] 3 5 3 2 5 3 6 5 6 6 5 6 3 6 2 6 4 4 1 6 6 5 1 6 3 2 1 1 3 2 2 4 6 6 1 1 5 6 4 1 5 2 4 5 6 3 ## [645] 4 5 6 6 1 3 3 3 1 2 4 6 4 3 2 6 6 2 6 2 4 1 2 1 6 2 3 2 6 6 6 2 2 4 3 6 4 3 3 1 5 6 3 3 6 1 ## [691] 4 4 5 4 3 6 3 1 2 5 6 1 5 4 4 1 4 2 1 4 3 3 1 5 1 6 2 6 4 3 3 1 1 1 3 5 2 5 6 3 3 6 1 1 5 3 ## [737] 2 5 5 2 4 4 5 4 3 5 4 5 6 1 5 5 6 1 3 2 2 1 5 4 3 6 4 5 2 2 6 3 1 2 1 4 1 1 6 2 4 3 4 1 4 3 ## [783] 1 1 6 2 3 1 3 6 2 1 6 1 5 2 5 4 4 4 6 3 5 3 5 4 1 1 3 2 5 2 3 5 6 4 4 4 5 6 1 5 6 6 3 3 5 1 ## [829] 4 5 6 2 5 5 4 4 4 1 6 3 5 6 1 3 6 2 3 3 5 4 1 2 2 6 2 3 3 1 4 5 3 5 5 1 4 4 3 1 5 2 1 3 3 3 ## [875] 5 4 1 4 6 5 1 5 3 4 1 3 1 2 1 6 3 3 4 3 5 6 2 1 3 4 This list of values is not particularly informative, perhaps we should make a histogram with the frequency at which each side came up. We can do this with both the hist() function or ggplot’s geom_histogram() function. Because ggplot() is somewhat more complex for this example, I’ll leave it for later. hist(outcomes, breaks = 0:6) As is always the case with random, even after 900 tosses, it’s still not perfect, but it approaches this. This approach is a nice segue to another distribution, the probability distribution. Let’s say we’re still not bored and want to experiment some more. Let’s say you take a second dice and throw both dice, you sum the result, and you repeat this another 99 times. The code for this is somewhat more complicated. It’s simple for the first toss though, we use same function as before. dice &lt;- 2 outcome &lt;- sample(sides, dice, replace = TRUE) # replace = TRUE is necessary to # tell the function that we accept that the same outcome may occur multiple times print(outcome) ## [1] 2 6 Then we sum the outcome with the sum() function. outcome_sum &lt;- sum(outcome) print(outcome_sum) ## [1] 8 So what are the chances that any sum of the two dices comes up? There’s in total 11 possible outcomes. The lowest possible sum is 2, if both dice turn up with 1. The highest possible outcome is 12, when both dice show 6. There’s only one possible cominbation to reach either of these outcomes. The outcome of 4, can be obtained in more than one way. Dice 1 shows 1, dice 2 shows 3, or vice versa, or both dice show 2. Remember that there’s in total 36 possible outcomes (6 possibilties for either dice in any possible cominbation, 6x6). So the possibility of obtaining a sum of 12 is 1/36, since there’s only one possible combination this can be achieved. For the outcome of 4, it’s 3/36, as a combination of [1,3], [2,2], or [3,1]. Using the same logic, there’s 6 different ways of getting the result of 7. This also makes 7 the most likely sum to come up, since there’s the largest number of combination to achieve this result. So let’s say we want to throw a pair of dice 100 times, to simulate this in R, we put it into a replicate() function. And instead of the individual steps we used before, we now group the functions together in one line, which effectively just means wrapping the sample() function in a sum() function. We want to replicate this 100 times. n_replications &lt;- 100 outcomes &lt;- replicate(n = n_replications, expr = sum(sample(sides, dice, replace = TRUE))) print(outcomes) ## [1] 7 5 12 7 6 5 7 7 4 4 6 9 11 6 6 7 6 9 8 7 7 7 7 8 6 11 10 7 3 7 8 ## [32] 10 7 7 5 6 12 5 6 4 6 6 8 4 4 9 5 5 3 8 8 9 6 8 9 3 5 3 11 8 2 8 ## [63] 7 7 5 5 3 6 4 7 5 7 7 6 7 7 12 5 10 9 2 9 9 3 5 10 9 6 6 8 5 6 2 ## [94] 8 5 7 6 6 7 4 Again, we can plot these values on histogram. hist(outcomes, breaks = 1:12) The distribution that this results in is the probability distribution, as you can see, the distribution is not yet identical to what we predicted theoretically. The probability distribution denotes the probabiltiy of different outcomes of an experiment. Since there’s more different ways of obtaining a sum of 6, 7, or 8 than there’s is to get a sum of 3, or 12 If you sample enough experiments, the probability distribution will tend to resemble the distribution that we expected from the theory, also known as the normal distribution (or Gaussian distribution). A normal distribution is a continuous probabilty distrbution. A set of 100 experiments is a good start, but when you run 1000 or 10.000 experiments, the distribution resembles more and more a perfect normal distribution. n_replications &lt;- 10000 outcomes &lt;- replicate(n = n_replications, expr = sum(sample(sides, dice, replace = TRUE))) hist(outcomes, breaks = 1:12) In the figure below, I’ve included an animation of how ten different random experiments plot on top of the theoretical probability distribution: ## Rendering [&gt;---------------------------------------------------------------] at 5 fps ~ eta: 20s Rendering [=&gt;------------------------------------------------------------] at 4.8 fps ~ eta: 20s Rendering [=&gt;------------------------------------------------------------] at 4.9 fps ~ eta: 19s Rendering [==&gt;-----------------------------------------------------------] at 4.9 fps ~ eta: 19s Rendering [===&gt;----------------------------------------------------------] at 4.9 fps ~ eta: 19s Rendering [====&gt;---------------------------------------------------------] at 4.9 fps ~ eta: 19s Rendering [=====&gt;--------------------------------------------------------] at 4.9 fps ~ eta: 18s Rendering [======&gt;-------------------------------------------------------] at 4.9 fps ~ eta: 18s Rendering [=======&gt;--------------------------------------------------------] at 5 fps ~ eta: 18s Rendering [=======&gt;------------------------------------------------------] at 4.9 fps ~ eta: 18s Rendering [========&gt;-----------------------------------------------------] at 4.9 fps ~ eta: 18s Rendering [========&gt;-----------------------------------------------------] at 4.9 fps ~ eta: 17s Rendering [=========&gt;----------------------------------------------------] at 4.9 fps ~ eta: 17s Rendering [==========&gt;---------------------------------------------------] at 4.9 fps ~ eta: 17s Rendering [===========&gt;--------------------------------------------------] at 4.8 fps ~ eta: 17s Rendering [============&gt;-------------------------------------------------] at 4.8 fps ~ eta: 16s Rendering [=============&gt;------------------------------------------------] at 4.8 fps ~ eta: 16s Rendering [==============&gt;-----------------------------------------------] at 4.8 fps ~ eta: 16s Rendering [===============&gt;----------------------------------------------] at 4.8 fps ~ eta: 15s Rendering [================&gt;---------------------------------------------] at 4.8 fps ~ eta: 15s Rendering [=================&gt;--------------------------------------------] at 4.8 fps ~ eta: 15s Rendering [==================&gt;-------------------------------------------] at 4.8 fps ~ eta: 14s Rendering [===================&gt;------------------------------------------] at 4.8 fps ~ eta: 14s Rendering [====================&gt;-----------------------------------------] at 4.8 fps ~ eta: 14s Rendering [=====================&gt;----------------------------------------] at 4.8 fps ~ eta: 13s Rendering [======================&gt;---------------------------------------] at 4.8 fps ~ eta: 13s Rendering [=======================&gt;--------------------------------------] at 4.8 fps ~ eta: 13s Rendering [========================&gt;-------------------------------------] at 4.8 fps ~ eta: 13s Rendering [========================&gt;-------------------------------------] at 4.8 fps ~ eta: 12s Rendering [=========================&gt;------------------------------------] at 4.8 fps ~ eta: 12s Rendering [==========================&gt;-----------------------------------] at 4.8 fps ~ eta: 12s Rendering [===========================&gt;----------------------------------] at 4.8 fps ~ eta: 11s Rendering [============================&gt;---------------------------------] at 4.8 fps ~ eta: 11s Rendering [=============================&gt;--------------------------------] at 4.8 fps ~ eta: 11s Rendering [==============================&gt;-------------------------------] at 4.8 fps ~ eta: 10s Rendering [===============================&gt;------------------------------] at 4.8 fps ~ eta: 10s Rendering [================================&gt;-----------------------------] at 4.8 fps ~ eta: 10s Rendering [=================================&gt;----------------------------] at 4.8 fps ~ eta: 9s Rendering [==================================&gt;---------------------------] at 4.8 fps ~ eta: 9s Rendering [===================================&gt;--------------------------] at 4.8 fps ~ eta: 9s Rendering [====================================&gt;-------------------------] at 4.8 fps ~ eta: 9s Rendering [====================================&gt;-------------------------] at 4.8 fps ~ eta: 8s Rendering [=====================================&gt;------------------------] at 4.8 fps ~ eta: 8s Rendering [======================================&gt;-----------------------] at 4.8 fps ~ eta: 8s Rendering [=======================================&gt;----------------------] at 4.8 fps ~ eta: 7s Rendering [========================================&gt;---------------------] at 4.8 fps ~ eta: 7s Rendering [=========================================&gt;--------------------] at 4.8 fps ~ eta: 7s Rendering [==========================================&gt;-------------------] at 4.8 fps ~ eta: 6s Rendering [===========================================&gt;------------------] at 4.8 fps ~ eta: 6s Rendering [============================================&gt;-----------------] at 4.8 fps ~ eta: 6s Rendering [=============================================&gt;----------------] at 4.8 fps ~ eta: 5s Rendering [==============================================&gt;---------------] at 4.8 fps ~ eta: 5s Rendering [===============================================&gt;--------------] at 4.8 fps ~ eta: 5s Rendering [================================================&gt;-------------] at 4.8 fps ~ eta: 4s Rendering [=================================================&gt;------------] at 4.8 fps ~ eta: 4s Rendering [==================================================&gt;-----------] at 4.8 fps ~ eta: 4s Rendering [===================================================&gt;----------] at 4.8 fps ~ eta: 3s Rendering [====================================================&gt;---------] at 4.8 fps ~ eta: 3s Rendering [=====================================================&gt;--------] at 4.8 fps ~ eta: 3s Rendering [======================================================&gt;-------] at 4.8 fps ~ eta: 2s Rendering [=======================================================&gt;------] at 4.8 fps ~ eta: 2s Rendering [========================================================&gt;-----] at 4.8 fps ~ eta: 2s Rendering [=========================================================&gt;----] at 4.8 fps ~ eta: 1s Rendering [==========================================================&gt;---] at 4.8 fps ~ eta: 1s Rendering [===========================================================&gt;--] at 4.8 fps ~ eta: 1s Rendering [============================================================&gt;-] at 4.8 fps ~ eta: 0s Rendering [==============================================================] at 4.8 fps ~ eta: 0s ## Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) ## Finalizing encoding... done! Some statistical tests that we cover later require that the data is normally distributed in order to output a reliable statistic. We can test whether or not the set of data we have is normally distributed by using a Shapiro-Wilk’s normality test. This test compares the distribution of our data with a perfectly normally distributed set of data with the same mean as our data. The R function to perform a Shapiro-Wilk’s test is shapiro.test(). The output of this function is a p-value indicating whether our data is significantly different from a normal distribution. So if our data is roughly normally distributed, the p-value is larger than 0.05, if our data is not normally distributed, it is significantly different from a normal distribution and the p-value will be below 0.05. Let’s try it on the scores from the history exam we used earlier! shapiro.test(data$scores) ## ## Shapiro-Wilk normality test ## ## data: data$scores ## W = 0.84665, p-value = 0.1841 shapiro.test(data_highschool$scores) ## ## Shapiro-Wilk normality test ## ## data: data_highschool$scores ## W = 0.96086, p-value = 0.814 The p-value is higher than 0.05 for both datasets, so that means that we can assume the data is normally distributed. "],
["basic-statistics.html", "6 Basic Statistics 6.1 Hypotheses and the T-test 6.2 ANOVA 6.3 Correlation", " 6 Basic Statistics 6.1 Hypotheses and the T-test Let’s go back to the example with the history exam. We’ve had two groups of students take the same exam. A group of students from a primary school, and a group of students from a high school. Before we started the exam, we predicted that the high school students would score higher on average. That was our hypothesis. In hypothesis testing, as a rule there’s always two hypothesis, the null-hypothesis and the alternative hypothesis. The alternative hypothesis (also referred to as Ha) is the statement we just made: “high school students score on average higher on this test than primary school students”. The null hypothesis (or H0) is defined as the negation of the alternative hypothesis. In this case the null hypothesis is something like “the high school students do not score higher on average than the primary school students”, or “there is no difference in the average score between the primary school stundents and the high school students”. We can test whether this hypothesis is true with a statistical test. How exciting! The most appropriate test here is a T-test. We choose a T-test here because we have two groups that we want to compare, and the data in each group is a set of numbers. The function in R to perform a T-test is aptly named t.test(). This function takes either one or two sets of data. If you supply only one set of data, then the function compares that data to a normal distribution centered around 0. The T-test is only reliable when the data is normally distributed, but we showed earlier that this was the case, so we can move on and perform the test. There are a few options that we can set in the t.test() function. Since our hypothesis stated that we exptect primary school students to score on average lower than the high school students, we don’t use a two-tailed test, but rather a one-tailed test. We can set this in the alternative field. t.test(data$scores, data_highschool$scores, alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: data$scores and data_highschool$scores ## t = -1.6868, df = 4.3495, p-value = 0.08058 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.4940209 ## sample estimates: ## mean of x mean of y ## 6.6 8.7 There’s a few more options that we can set, and I’ll quickly go over them below, R has default settings for a number of these options, so it’s not always necessary to set these explicitely. t.test(data$scores, data_highschool$scores, # The two sets of data to compare alternative = &quot;less&quot;, # Other options are &quot;two.sided&quot; or &quot;greater&quot; mu = 0, # Relevant if we supplied only one set of values, otherwise # this sets the mean value for the perfect normal distribution paired = FALSE, # Is this a paired T-test? Alternative input: TRUE var.equal = FALSE, # Can equal variance be assumed? Alternative: TRUE conf.level = 0.95) # Confidence level ## ## Welch Two Sample t-test ## ## data: data$scores and data_highschool$scores ## t = -1.6868, df = 4.3495, p-value = 0.08058 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.4940209 ## sample estimates: ## mean of x mean of y ## 6.6 8.7 Here we find that there is no significant difference in the scores between primary school and high school students. This is somewhat surprising, since the values are very different. The reason we didn’t get a significant result here is likely because both groups contained only five students, and this is very low for a statistical test. We should definitely include more participants. Ideally, we would perform a power analysis before we even started the experiment, but we’ll get to that later. So let’s just for the sake of argument, say that we tested 10 students in each group, what would it look like then. The new datasets are as follow: data &lt;- data.frame( names = c(&quot;Lucas&quot;, &quot;Linn&quot;, &quot;Thomas&quot;, &quot;Sara&quot;, &quot;Anna&quot;, &quot;Hassan&quot;, &quot;Ingvild&quot;, &quot;Jostein&quot;, &quot;Yuri&quot;, &quot;Olav&quot;), scores = c(7.5, 8, 2, 6.5, 9, 5, 6, 8, 6.5, 7.5) ) data_highschool &lt;- data.frame( names = c(&quot;Alex&quot;, &quot;Solveig&quot;, &quot;Ivar&quot;, &quot;Yasmin&quot;, &quot;Tobbe&quot;, &quot;Christine&quot;, &quot;Johannes&quot;, &quot;Mo&quot;, &quot;Trude&quot;, &quot;Trygve&quot;), scores = c(8.5, 9, 8.5, 9.5, 8, 8.5, 9, 8.5, 9.5, 8) ) print(data) ## names scores ## 1 Lucas 7.5 ## 2 Linn 8.0 ## 3 Thomas 2.0 ## 4 Sara 6.5 ## 5 Anna 9.0 ## 6 Hassan 5.0 ## 7 Ingvild 6.0 ## 8 Jostein 8.0 ## 9 Yuri 6.5 ## 10 Olav 7.5 print(data_highschool) ## names scores ## 1 Alex 8.5 ## 2 Solveig 9.0 ## 3 Ivar 8.5 ## 4 Yasmin 9.5 ## 5 Tobbe 8.0 ## 6 Christine 8.5 ## 7 Johannes 9.0 ## 8 Mo 8.5 ## 9 Trude 9.5 ## 10 Trygve 8.0 You rerun the same code we used earlier to see if the mean score is still the same for both groups, how much the standard deviation has changed, and whether the data is still roughly normally distributed. If we run the same code for the t-test again now, we get a whole different result: t.test(data$scores, data_highschool$scores, alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: data$scores and data_highschool$scores ## t = -3.2318, df = 10.315, p-value = 0.004325 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.9259259 ## sample estimates: ## mean of x mean of y ## 6.6 8.7 Again, this is just for educational purpose now. In a real scientific setting, you would determine the number of experiments you need to perform up front and report that before you start experimenting. It is generally considered bad practice to keep testing and testing until you get the result you wanted all along. Back to the result, the ouput from the t.test() function is pretty self-explanatory. For this example, we accept a p-value lower than 0.05 as a significant result. Make sure to also look at the t-statistic and the degrees of freedom. 6.2 ANOVA Let’s say you design a new experiment, and now you also add a third group of students, all university students in the first year of their bachelor in history. Try running the same tests again for this group. You’ll find that this set of data is still rougly normally distributed and the mean score lies higher than the other two groups. Their results look like this: print(data_university) ## names scores ## 1 Johnny 9.0 ## 2 Synne 9.5 ## 3 Petter 10.0 ## 4 Yusuf 10.0 ## 5 Christy 9.0 ## 6 Olek 9.5 ## 7 Abdullah 9.5 ## 8 Jan 10.0 ## 9 Emilie 10.0 ## 10 Roger 8.5 Since we now have three data frames to work with, it starts to become a little messy, that’s why I combined all three data frames into one, and added a column specifying the group each individual student belonged to: print(data_comb) ## names group scores ## 1 Lucas Primary School 7.5 ## 2 Linn Primary School 8.0 ## 3 Thomas Primary School 2.0 ## 4 Sara Primary School 6.5 ## 5 Anna Primary School 9.0 ## 6 Hassan Primary School 5.0 ## 7 Ingvild Primary School 6.0 ## 8 Jostein Primary School 8.0 ## 9 Yuri Primary School 6.5 ## 10 Olav Primary School 7.5 ## 11 Alex High School 8.5 ## 12 Solveig High School 9.0 ## 13 Ivar High School 8.5 ## 14 Yasmin High School 9.5 ## 15 Tobbe High School 8.0 ## 16 Christine High School 8.5 ## 17 Johannes High School 9.0 ## 18 Mo High School 8.5 ## 19 Trude High School 9.5 ## 20 Trygve High School 8.0 ## 21 Johnny University 9.0 ## 22 Synne University 9.5 ## 23 Petter University 10.0 ## 24 Yusuf University 10.0 ## 25 Christy University 9.0 ## 26 Olek University 9.5 ## 27 Abdullah University 9.5 ## 28 Jan University 10.0 ## 29 Emilie University 10.0 ## 30 Roger University 8.5 If we now want to test whether there is a difference in scores between the three groups, we cannot use a T-test anymore. To compare the mean value of multiple groups, we use an Analysis of Variance, better known as an ANOVA. Performing an ANOVA in R is twice as complicated as a T-test in that running an ANOVA takes two lines of code instead of one. First we need to set up the model that we want to test with possible covariates (which we don’t have at the moment). The function for setting up the ANOVA formula is aov(). model &lt;- aov(scores ~ group, data = data_comb) Then we can run the actual ANOVA command, which is called anova(). This function takes the model and runs the ANOVA statical test. It prints again a list of pretty straight-forward output. In most cases, you’re just interested in the line with the grouping variable, we can ignore the row with “Residuals” for now. anova(model) ## Analysis of Variance Table ## ## Response: scores ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 44.867 22.433 14.956 4.249e-05 *** ## Residuals 27 40.500 1.500 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We find that there is a significant effect of the grouping variable on the score. And the effect size (denoted by the F-value) is high too. Success! But the output we now have is the bare minimum we need to report in articles etc., but we can do better. Remember the T-test we ran earlier? That was a comparison between primary school students and high school students, but now that we have three groups we can make two more comparisons, [High School-Universtity], and [Primary School-University]. You could do this with independent T-tests, but it’s important to remember that we then effectively run 3 T-tests within the same dataset. A T-test is appropriate when you have independent groups, but when we run this three-way T-test, the tests are not really independent anymore, since each group will appear twice in each comparison. And it’s very imporant that we take this into account. So what do we do when we want to run T-tests within a comparison of three groups to see what drives the effect we observed when we ran the ANOVA? There’s a function called the pairwise.t.test(), which does exactly what we decribed above, but it’s not what we want in this case. What we do want is to run a post-hoc test! Again, this is just a simple single line of code. There’s multiple options for post-hoc available in R, but we use the most common one is the Tukey’s Honest Significant Differences test, commonly known as Tukey’s post-hoc test. Tukey’s post-hoc test is only relevant when you want to run pairwise T-tests on all possible combinations of groups, as we do here. The function for this in R is TukeyHSD(). It requires only the ANOVA model we created earlier using the aov() function. Since there’s now three groups in the statistical test, we cannot specify whether we want a one-tailed or two-tailed test. So for comparison, let’s run the T-test again for the comparison between primary school and high school students, but let it be a two-tailed test this time. Let’s do it: t.test(data$scores, data_highschool$scores, alternative = &quot;two.sided&quot;) ## ## Welch Two Sample t-test ## ## data: data$scores and data_highschool$scores ## t = -3.2318, df = 10.315, p-value = 0.00865 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.5418444 -0.6581556 ## sample estimates: ## mean of x mean of y ## 6.6 8.7 Now that we have that for comparison, let’s now run Tukey’s post-hoc test: TukeyHSD(model) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = scores ~ group, data = data_comb) ## ## $group ## diff lwr upr p adj ## High School-Primary School 2.1 0.741967 3.458033 0.0019198 ## University-Primary School 2.9 1.541967 4.258033 0.0000401 ## University-High School 0.8 -0.558033 2.158033 0.3252891 The ouput from this is relatively simple, it shows the output from the pairwise tests it performed, with the mean difference, the upper and lower bounds, and the adjusted p-value. Note that the p-value for the comparison between primary school students and high school students is higher in the post-hoc test than in the T-test. This is because the post-hoc test automatically corrects for multiple testing. I’ll dedicate an entire section to the multiple testing problem and why it’s important, and why it’s one of the most severe errors made in statistics. Unfortunately, it’s also a very common mistake. Be also aware that there’s different ways of going about calculating ANOVA. In order to perform the calculation, one needs to calculate a parameter called the “sum of squares”. There’s three different ways of calculating the sum of squares, Type I, Type II, and Type III. We might go more in-depth later on what the difference in between these, but the important part here is that one might get different results based on the type used and SPSS and R have different default settings on how to calculate it. So if you’re trying to compare your results from SPSS with the results from R, then you might get different results. So be aware! If you want to use Type III sum of squares in R, you can run the ANOVA implementation in the {afex} package. If your data is normally distributed and the factors are orthogonal, then the three types will give the same result, but if not, then keep this in mind! 6.3 Correlation Now for the last statistical test that is quite common in clinical research, the correlation. Let’s say we take the same dataset, and we also gathered data on the parent’s income. Then we can see how the parents income affect their children’s performance in school. While this may not have a direct effect, wealthy parents might be more able to afford tutoring, and wealthy parents are more likely to live in nice neighbourhoods and provide a good situation for the child to do homework. This means that income might be a proxy for other factors. Let’s say the new data looks like this: ## [1] 1850000 650000 650000 400000 1250000 450000 550000 2000000 500000 450000 950000 ## [12] 500000 1500000 950000 1650000 2000000 850000 2000000 750000 700000 1650000 600000 ## [23] 1550000 1500000 1050000 1050000 1800000 900000 1900000 650000 So now we can run a correlation. The simplest way to do this is by using the cor.test() function. It behaves the same was as the t.test() function, but now it takes two continuous variables as input. Now we use a two-sided hypothesis, and want to use Spearman’s Rank test, which is set in the method option in the cor.test() function. Let’s do it: cor.test(data_comb$scores, data_comb$income_parents, method = &quot;spearman&quot;) ## Warning in cor.test.default(data_comb$scores, data_comb$income_parents, : Cannot compute exact p- ## value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: data_comb$scores and data_comb$income_parents ## S = 2745.3, p-value = 0.03349 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.3892626 So we can see that there is an effect of parent’s income on test scores, and it’s significant too. But we’ve overlooked one big thing, that the students in the dataset that scored the highest were the university students, and it is probable that there might be a bias in the dataset since university students are maybe more likely have wealthy parents, than primary school students or high school students. We want to correct for this. Instead of manually running the test three times, I would recommend to “regress out” the group variable from the scores. We do this by running a linear model with the group as the covariate to regress out and the look at the residual scores. We can “residualize” the data as it’s called by using the lm() function. The input for this is similar to that of the aov() option. Just to compare I will run it twice, once correcting for group, and once without covariate. lm_model_nocov &lt;- lm(scores ~ income_parents, data = data_comb) lm_model &lt;- lm(scores ~ income_parents + group, data = data_comb) data_comb$scores_resid_nocov &lt;- lm_model_nocov$residuals data_comb$scores_resid &lt;- lm_model$residuals The mean of a residualized variable is by definition 0, but doesn’t have to be normally distributed. In our case here, both residualized variables aren’t actually normally distributed since outliers in the regular data will be outliers in the residualized data too. Let’s check the boxplot: boxplot(data_comb$scores_resid) So now that we’ve “regressed out” the grouping variable, we now have a cleaner set of data to estimate the effect of parental income on test scores with. Let’s do it using the same method we used before: cor.test(data_comb$scores_resid, data_comb$income_parents, method = &quot;spearman&quot;) ## Warning in cor.test.default(data_comb$scores_resid, data_comb$income_parents, : Cannot compute ## exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: data_comb$scores_resid and data_comb$income_parents ## S = 4956.8, p-value = 0.5891 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.1027299 That’s very interesting, it seems that when not correcting for the student’s current educiaton level, there is a significant correlation between income and the student’s test scores, but when you correct for the group the students were in, this relationship disappears. It seems that the grouping variable was driving the assocation in this case. Let’s have another look at the residuals. When we regressed out the grouping variable, I also did it once without correcting for the group. I just wanted to show the difference between the two to illustrate the effect that regressing categorical variables has on the data. Hopefully I’ll convince you to correct for variables often and early in the process. Let’s compare the distributions of both the corrected and uncorrected data. boxplot(data_comb$scores_resid, data_comb$scores_resid_nocov, names = c(&quot;Corrected&quot;, &quot;Uncorrected&quot;)) The difference is subtle, but you can see that the first boxplot has a narrower confidence interval and smaller error bars. So both boxplots contain data from the same observation, so we can also see how the data points change for each group when we take the uncorrected residualized data and the corrected residualized data. Let’s plot it like a scatter plot and plot the income of the parents on the x-axis, and the residualized scores on the y-axis. The small dots indicate the uncorrected data, and the larger dots indicate the data corrected for group. You can see that the adjustment direction is consistent across the three groups, but the size of the adjustment varies across individuals. You may also notice that the data points on the left side of the plot are mostly primary school children, while the other two groups are more distributed across the x-axis. Let’s do the statistics again. cor.test(data_comb$income_parents, data_comb$scores_resid, method = &quot;spearman&quot;) ## Warning in cor.test.default(data_comb$income_parents, data_comb$scores_resid, : Cannot compute ## exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: data_comb$income_parents and data_comb$scores_resid ## S = 4956.8, p-value = 0.5891 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.1027299 Right! So now the significant effect has dissappeared, just because we regressed out the education levels from the scores. Another way of getting to this conclusion is to look at the raw output from the lm() model. Here again the summary() function comes in useful. Remember we saved the output from the lm() function in a variable called lm_model, we can use that now to look at what the linear model said about the predictor of scores. summary(lm_model) ## ## Call: ## lm(formula = scores ~ income_parents + group, data = data_comb) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4772 -0.4321 0.1354 0.6261 2.1954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.662e+00 5.214e-01 14.696 4.16e-14 *** ## income_parents 5.457e-07 4.261e-07 1.281 0.212 ## group.L 1.900e+00 4.004e-01 4.745 6.58e-05 *** ## group.Q -4.795e-01 3.849e-01 -1.246 0.224 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.21 on 26 degrees of freedom ## Multiple R-squared: 0.5537, Adjusted R-squared: 0.5022 ## F-statistic: 10.75 on 3 and 26 DF, p-value: 8.913e-05 While there is a significant effect of group (which we already knew from the ANOVA we ran earlier), we can see that the income from the parents is not a significant predictor of test scores in this dataset. At this point you might go, “wait, why does this regression model also tell us stuff the ANOVA told us?”. The short answer is that any ANOVA is at it’s core basically a linear model, why and how that works is somewhat technical and complicated, and I won’t go into it now. For most practical purposes, you may continue using ANOVAs and regression models as if they’re two separate things entirely. Even if it was just because post-hoc tests are built around the structure of an ANOVA, and not linear models. Okay, now that we’ve covered some of the most common statistical tests in clinical research, I think it’s important to talk about p-values, and why some of the nice p-values lower than 0.05 we found earlier are pretty useless for most purposes. "],
["probability-values.html", "7 Probability values", " 7 Probability values In statistics there’s two types of errors one can make: Type I and Type II. Type I errors refer to false positives, Type II errors refer to false negatives. A famous table explaining how Type I and Type II errors relate to the hypotheses is shown below: Outcome of test H0 is true H0 is false Do not reject H0 Correct finding Type II error Reject H0 Type I error Correct finding The p-value is nothing more than a probability, a percentage indicating the chance that the result we obtained was due to random chance. In a coin flip, the probability of either side coming up is 50%, or a probability of 0.5, I would also refer back to the dice experiment we discussed earlier to revisit probability distributions. Every now and then I overhear someone saying a variation of the following sentence “this might be a major finding, it’s very significant!”. While I can appreciate the enthousiasm, it’s important to stress that one cannot conflate the p-value with a size of effect. All the p-value states is the probability that the effect observed was due to chance. In most (medical) sciences, the probabilty they accept is 5%, or one out of 20 chance. But when we establish that we accept a 5% probability that the result we obtain is due to chance, then we have an issue when we run multiple tests. Why? "],
["the-multiple-testing-problem.html", "8 The Multiple Testing Problem", " 8 The Multiple Testing Problem Since we accept a 5% probability at every test that the effect we measure is due to chance, what happens when we run 2 tests? Then the probability that the effects we measured is due to chance becomes 0.05 + 0.05 = 0.1, or 10%. So as you can imagine, this will increase the likelihood of finding a result when the null hypothesis is true quite quickly, i.e. we increase the chance of making a Type I error. So what do we do about it? If we still want to get a 5% chance that we get a false positive result, then we need to correct the signficance level of each test for the number of tests we’re going to do. There’s multiple ways we can do this, and I’ll get to that later. Some of these tests use a parameter called the Family-Wise Error Rate (FWER), this calculates the probility of obtaining at least one significant result due to chance. The function for this is incorporated in the {normentR} package, the function is called FWER(). So let’s say we want to calculate the FWER for 20 tests. We first need to load the {normentR} package, we do that via library(normentR). The FWER() function takes two inputs, the number of comparisons in n, and the significance level in alpha. We set the number of comparisons to 20, and the alpha level to 5%. library(normentR) FWER(n = 20, alpha = 0.05) ## [1] 0.6415141 We find that the FWER for 20 comparisons at significance level 5% is about equal to 64%. So there’s a 64% chance that when we run 20 comparisons, and we don’t chance the default significance level from 5%, and we don’t correct for multiple testing, we find at least one result that is statistically significant. Let’s try this again with 50 comparisons. FWER(n = 50, alpha = 0.05) ## [1] 0.923055 I’ve made a little calculator to visually show how the family-wise error rate changes based on the number of tests and the significance threshold. See how lowering the significance threshold (α-level) reduces your chance of a false positive dramatically, and how quickly the chance of at least one false positive increases with the number of comparisons. If you approach 200 comparisons, the FWER shows 100%, this is of course impossible, but I chose to include it anyway, since it’s 100% for all practical purposes. Now the probability of finding AT LEAST one significant result with 50 comparisons at the same parameters is more than 92%! Unfortunately, it’s not uncommon not to correct for multiple testing, and it’s a major contributer for experiments to fail to replicate. Especially when this is combined with p-hacking, which is the procedure in which researchers only report the significant results, and pretend that this was their hypothesis all along, neglecting to explicitely state the other hypotheses they tested but that didn’t yield significant results. It’s one of the gravest violations of good scientific practice. So how do we fix this? The most popular method to correct for multiple testing is adjusting the significance level for the number of comparisons in the experiment by dividing the significance level by the number of comparisons. This is called the Bonferroni correction. So let’s say we run 20 tests, then we need to divide the significance level (default at 5%) by 20, which makes 0.25%, and that’s the threshold at which we accept results to be statically significant. Let’s say the p-values look like this: print(pvalues) ## [1] 0.150 0.790 0.210 0.080 0.270 0.770 0.200 0.890 0.930 0.840 0.370 0.030 0.780 0.040 0.690 ## [16] 0.550 0.130 0.610 0.004 0.850 So we find that we have three tests that were significant! Great, let’s write the manuscript! But first we want to correct for multiple testing. Let’s do Bonferonni first, so then we divide the p-values by the number of comparisons. Obviously, there’s a convenient function in R to do this for us, p.adjust(). It takes the list of p-values and then the method by which you want to correct for multiple testing, in this case Bonferroni. p.adjust(pvalues, method = &quot;bonferroni&quot;) ## [1] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.60 1.00 0.80 1.00 1.00 1.00 1.00 ## [19] 0.08 1.00 The p.adjust() function takes multiple methods, another popular method is the False Discovery Rate (FDR) or the Holm or Hochberg method. Let’s try False Discovery Rate too, just for comparison. p.adjust(pvalues, method = &quot;fdr&quot;) ## [1] 0.5000000 0.9300000 0.5250000 0.4000000 0.6000000 0.9300000 0.5250000 0.9300000 0.9300000 ## [10] 0.9300000 0.7400000 0.2666667 0.9300000 0.2666667 0.9300000 0.9300000 0.5000000 0.9300000 ## [19] 0.0800000 0.9300000 We can see that the Bonferroni method is stricter than FDR, you should judge on a case-by-case basis what is the most appropriate method for you, but usually it’s the Bonferroni correction. I hope I’ve made abundantly clear why correcting for multiple testing is important, and why not correcting will result in false positive results! "],
["power-analysis.html", "9 Power Analysis", " 9 Power Analysis The section above describes in part how to reduce the chance of making a Type I error, now I’ll go over how to diminish the opportunity of making Type II errors. The most common reason researchers get a Type I error is failure to detect an effect that is present in the population, but couldn’t be measured in the sample because the sample size was too small. Therefore, it is generally recommended to perform a power analysis before running an experiment. A power analysis estimates the number of participants you need to test in order to make adequate claims about wether the hypotheses are true or false. It is somewhat complex algorithm that I won’t go into, but the way to perform a power analysis in R is quite simple. The {pwr} package has a few functions for this. So first we load that package via library(pwr), and then we estimate the power for the first experiment we used, comparing 5 primary school students with 5 high school students with a T-test. The function for this is called pwr.t.test(). And it takes a number of inputs. library(pwr) pwr.t.test(n = 10, # number of observations per sample d = 0.2, # precicted effect size (as Cohen&#39;s d) type = &quot;two.sample&quot;, # we have a two-sample T-test alternative = &quot;less&quot;, # alternative hypothesis sig.level = 0.05) # significance level ## ## Two-sample t test power calculation ## ## n = 10 ## d = 0.2 ## sig.level = 0.05 ## power = 0.01896144 ## alternative = less ## ## NOTE: n is number in *each* group This says that we have a little more than 1% power to detect a true effect. That’s not great! But how did I determine the predicted effect size (Cohen’s d)? In a normal setting, you would do your literature reseach and deduce it from there. In this case, I just made it up, that is bad practice generally. Side note: If you need to calculate Cohen’s d in R, there’s a function in the {normentR} package called cohens_d() that does exactly that. Just input the same columns you would input to the t.test() function and it gives you Cohen’s d for that test. Let’s try it on our data. We already loaded the {normentR} package earlier. cohens_d(data$scores, data_highschool$scores) ## [1] -1.445319 Back to the power analysis. What about the ANOVA? Do we have enough power to detect an effect with three groups? We have again 10 pariticipants in each group, and we want to detect an effect of 0.8, measured as the F-statistic. pwr.anova.test(k = 3, # number of groups n = 10, # number of observations in each group f = 0.8, # effect size (as F-statistic) sig.level = 0.05) # significance level ## ## Balanced one-way analysis of variance power calculation ## ## k = 3 ## n = 10 ## f = 0.8 ## sig.level = 0.05 ## power = 0.9676791 ## ## NOTE: n is number in each group Here we find that we have quite a bit more power! Now we have almost 97% power to detect the effect! Since power is defined as 1 minus the probability of a Type II error, the probability of rejecting the alternative hypothesis while the null hypothesis is false is a little more than 3%, that’s acceptable! So now that we’ve discussed how to reduce the risk of Type I and Type II, let’s move on to the fun stuff, making pretty plots! :D "],
["plotting.html", "10 Plotting 10.1 Basics &amp; Mapping 10.2 Types of plot 10.3 Layout and Style 10.4 Themes 10.5 Statistics in plots 10.6 NORMENT options 10.7 More examples 10.8 Repeated-Meausures ANOVA", " 10 Plotting There’s a number of built-in functions that we can use, and one of those we’ve used already, such as hist(). Others are also quite self-explanatory such as plot(), or barplot(). I’ll just show one, since this one is apparently very difficult to make in SPSS: the boxplot() function. Let’s make a boxplot of the three groups. The setup is almost identical to the aov() function we used before. boxplot(scores ~ group, data = data_comb) While this is already much better than the default output from SPSS, we can do better. And for that I want to introduce an immensely popular package. Once familiar with this package, you’ll recognize it on a great many manuscripts, posters and presentations: the {ggplot2} package. I’ll just create a plot first, and then I’ll go through how it works. library(ggplot2) ggplot(data = data_comb, mapping = aes(x = group, y = scores)) + geom_boxplot() Just to show what is possible, the next plot has been made with just the {ggplot2} package and the {ggpubr} package. I won’t show the code just yet, but we might get to this level later, but this is what a fully modified ggplot() plot might look like: 10.1 Basics &amp; Mapping Okay, so how do we go from the first simple plot to the second one? The functions in the {ggplot2} package effectively layer on top of each other. The first step is always a ggplot() function. Inside of this function you supply the data frame that contains the data you want to plot, and then you usually supply the mapping too. In the mapping you specify what you want on the x-axis, the y-axis, what the colors should represent (if anything), and there’s a few more options. You add the next layer by typing + at the end of the function. 10.2 Types of plot The second layer is usually the type of plot you want to use, in our last example this was geom_boxplot(), but other options include geom_point() (for scatterplots), geom_line() (for line plots), geom_bar() (for bar charts), and many more. And you can layer them, like I did in the last example, where I added a geom_jitter() first, and then a geom_boxplot() on top of that. Let’s follow along, now we have a call to ggplot() and a call to geom_jitter() and geom_boxplot(). Both these functions take a number of inputs. You can find a full list of options in all functions of R by typing ?&lt;functionname&gt;, e.g. ?geom_jitter. alpha in this context means the opacity of the color, width of geom_jitter() specifies the width along which the dots will be jittered. The width in both functions is the width along points will be scattered, or the width of the boxplot. outlier.shape determines the behavior of the outliers, outlier.shape = 4 changes the outlier to a little x. ggplot(data = data_comb, mapping = aes(x = group, y = scores, fill = group)) + geom_jitter(alpha = 0.5, width = 0.1) + geom_boxplot(alpha = 0.9, width = 0.5, outlier.shape = 4) 10.3 Layout and Style Next, we add a number of layers to make the plot look prettier and more informative. These layers contain information about what you want the axes labels to say, and the specific colors to use. There’s an infinite amount of options for the color palettes. There’s entire packages dedicated to make your colors resemble colors in the Harry Poter universe, the families from Game of Thrones, the Ghibli movies, or the palettes from the Wes Anderson movies. The last one is pretty fun, so let’s use that. These color palettes are stored in the {wesanderson} package. Axes labels can be set with the labs() function, and the behavior of the axes itself can be set with scale_x_discrete() or scale_y_continuous() in this particular case. I think the x-axis looks good already, but I would like to change the y-axes behavior. For the colors we have two options, either a color setting or a fill setting. With a boxplot, the color setting will change just the lines of the boxplot, the fill setting changes the fill and not the lines. Keep this in mind when building your plot to always be consistent with the functions that affect the color or fill. library(wesanderson) ggplot(data = data_comb, mapping = aes(x = group, y = scores, fill = group)) + geom_jitter(alpha = 0.5, width = 0.1) + geom_boxplot(alpha = 0.9, width = 0.5, outlier.shape = 4) + labs(x = NULL, # NULL in this context means &quot;no label&quot; y = &quot;Score&quot;, fill = NULL) + # the fill in this case refers to the title of the legend scale_y_continuous(breaks = seq(0,10,2), minor_breaks = seq(1,9,2), limits = c(0,10)) + scale_fill_manual(values = wes_palette(&quot;IsleofDogs1&quot;)) This is quite a bit better already, the different scale_ functions might be a bit confusing at first, but Google is a great help. The breaks and minor.breaks options in the scale_y_continuous() function determine where major and minor grid lines along the y-axis will be shown. The seq() function creates a sequence from the first input to the second input in steps determined in the third input. For instance: seq(1,9,2) ## [1] 1 3 5 7 9 seq(3,16,3) ## [1] 3 6 9 12 15 The limits option sets the absolute limits from as c(&lt;lower limit&gt;, &lt;upper limit&gt;), since it’s good practice to always include the 0 in your plots when possible, and the maximum score anyone could have gotten was 10, we set the limits at 0 and 10 (i.e. sometimes you want to give limits some more space to allow for the random jitter to exceed the strict limits, e.g. c(0, 10.2)). 10.4 Themes The last layers typically are about the general layout of the plot. This is where we set the theme of the plot and the specific settings. A popular theme is theme_minimal() or theme_bw(). But the {normentR} package has a specific theme that corresponds with the NORMENT style guide, it’s called theme_norment(). This deals with a number of small modifications in the theme. There’s a number of great themes out there, and I’ve tried to collect the best options from each into the theme_norment() function, it includes a dark-mode, quite a few options for the font type, size, etc., functions to remove the background (great for posters), and a few more. Further options that you want to change from the functions included in the theme_&lt;theme function&gt; can be set in the theme() function. This function has almost an infinite number of options. Googling it might help. ggplot(data = data_comb, mapping = aes(x = group, y = scores, fill = group)) + geom_jitter(alpha = 0.5, width = 0.1) + geom_boxplot(alpha = 0.9, width = 0.5, outlier.shape = 4) + labs(x = NULL, y = &quot;Score&quot;, fill = NULL) + scale_y_continuous(breaks = seq(0,10,2), minor_breaks = seq(1,9,2), limits = c(0,10)) + scale_fill_manual(values = wes_palette(&quot;IsleofDogs1&quot;)) + theme_norment() + # set the NORMENT theme theme( panel.grid.major.x = element_blank(), # remove the grid lines on the x-axis rect = element_rect(fill = &quot;transparent&quot;) # make plot transparent ) 10.5 Statistics in plots I’ve included it in the plot earlier, but I’ll skip the pairwise T-tests for now. It’s quite simple with the {ggpubr} package, which includes a stat_compare_means() function which can be added as a layer to the plot. To add the ANOVA statistic, we just need to add the function to the plot, and specify the method we want to use in the method option. I’ll usually add this layer right after specifying the types of plot. The position of the ANOVA label can be moved to a specific place on the plot via the label.x and label.y options. ggplot(data = data_comb, mapping = aes(x = group, y = scores, fill = group)) + geom_jitter(alpha = 0.5, width = 0.1) + geom_boxplot(alpha = 0.9, width = 0.5, outlier.shape = 4) + stat_compare_means(method = &#39;anova&#39;, label.x = 2.5, label.y = 1.5) + labs(x = NULL, y = &quot;Score&quot;, fill = NULL) + scale_y_continuous(breaks = seq(0,10,2), minor_breaks = seq(1,9,2), limits = c(0,10)) + scale_fill_manual(values = wes_palette(&quot;IsleofDogs1&quot;)) + theme_norment() + theme( panel.grid.major.x = element_blank(), rect = element_rect(fill = &quot;transparent&quot;) ) 10.6 NORMENT options At the risk of sounding like I’m trying to chove the {normentR} package down your throat, I do want to measure some of the plotting from the {normentR} package. It has a specific function that sets the colors in the plot to NORMENT colors. It also has a function to add the NORMENT logo to any plot (called show_norment_logo()), which is great for presentations or sharing of figures on social media. Here I’ve created an example using some of these functions. plot &lt;- ggplot(data = data_comb, mapping = aes(x = group, y = scores, fill = group)) + geom_jitter(alpha = 0.5, width = 0.1) + geom_boxplot(alpha = 0.9, width = 0.5, outlier.shape = 4) + stat_compare_means(method = &#39;anova&#39;, label.x = 2.5, label.y = 1.5) + labs(x = NULL, y = &quot;Score&quot;, fill = NULL) + scale_y_continuous(breaks = seq(0,10,2), minor_breaks = seq(1,9,2), limits = c(0,10)) + scale_fill_norment(discrete = TRUE, palette = &quot;mixed&quot;) + theme_norment() + theme( panel.grid.major.x = element_blank(), rect = element_rect(fill = &quot;transparent&quot;) ) show_norment_logo(plot) The package also contains a function to move the axis to a specific place on the plot, which is useful for ERP plotting for instance, this function is called shift_axes(). The color palettes included in the {normentR} package can be found using show_norment_palette(), which creates an image with the organization of different palettes. show_norment_palette() You can create your own NORMENT palette by combining the colors manually. You can see the available colors by using the norment_colors command, which prints a list of the names and the HEX codes. You can create your own palette as shown below. norment_colors ## purple light purple magenta blue light blue green yellow ## &quot;#5c2483&quot; &quot;#814994&quot; &quot;#e5007d&quot; &quot;#0081c9&quot; &quot;#3CA2E6&quot; &quot;#21a695&quot; &quot;#d3d800&quot; ## black grey light grey white ## &quot;#000000&quot; &quot;#384044&quot; &quot;#cccccc&quot; &quot;#ffffff&quot; my_norment &lt;- c(norment_colors[[&quot;purple&quot;]], norment_colors[[&quot;green&quot;]], norment_colors[[&quot;yellow&quot;]]) print(my_norment) ## [1] &quot;#5c2483&quot; &quot;#21a695&quot; &quot;#d3d800&quot; I will admit that these colors are not great for individuals with colorblindness, or for printing in greyscale. There are some great packages available that will take care of this very elegantly, particularly the {scico} package, that can be found here. 10.7 More examples So now I want to go into a few more examples. For this we need to bring out the powerhouse of R, the {tidyverse} package. The {tidyverse} package contains a number of packages within to help move through your analysis in a more organized, easier, and generally better way. The {tidyverse} package is basically a collection of packages that include {ggplot2}, {readr} (to load data more efficiently), {dplyr} (to more effectively clean your data), and {tidyr} (to help organize your data in a tidy way), I might dedicate an entire section to the {tidyverse} later, but we’ll use some functions already now. Let’s try some things out with this using some data from Gapminder. Gapminder data is also stored in the {gapminder} package, Let’s say we’re only interested in data from Norway, so we filter only the data from Norway. Let’s also select all data from 2007 from all countries. The names() function lists the names of columns. The head() function prints only the first few rows of the data frame instead of the full data frame like print() does. library(tidyverse) library(gapminder) names(gapminder) # column names of the dataset ## [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; data_norway &lt;- filter(gapminder, country == &quot;Norway&quot;) # select only Norway data_2007 &lt;- filter(gapminder, year == 2007) # select only 2007 print(data_norway) # show all rows of data_norway ## # A tibble: 12 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Norway Europe 1952 72.7 3327728 10095. ## 2 Norway Europe 1957 73.4 3491938 11654. ## 3 Norway Europe 1962 73.5 3638919 13450. ## 4 Norway Europe 1967 74.1 3786019 16362. ## 5 Norway Europe 1972 74.3 3933004 18965. ## 6 Norway Europe 1977 75.4 4043205 23311. ## 7 Norway Europe 1982 76.0 4114787 26299. ## 8 Norway Europe 1987 75.9 4186147 31541. ## 9 Norway Europe 1992 77.3 4286357 33966. ## 10 Norway Europe 1997 78.3 4405672 41283. ## 11 Norway Europe 2002 79.0 4535591 44684. ## 12 Norway Europe 2007 80.2 4627926 49357. head(data_2007) # show the first few rows of data_2007 ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2007 43.8 31889923 975. ## 2 Albania Europe 2007 76.4 3600523 5937. ## 3 Algeria Africa 2007 72.3 33333216 6223. ## 4 Angola Africa 2007 42.7 12420476 4797. ## 5 Argentina Americas 2007 75.3 40301927 12779. ## 6 Australia Oceania 2007 81.2 20434176 34435. So let’s do some plots again! First, I’m interested in how the life expectancy changed in Norway from the earliest recording to the most recent one in the dataset. I want to make a line plot for this. Let’s do it: ggplot(data = data_norway, mapping = aes(x = year, y = lifeExp)) + geom_line() So this is the simplest ggplot we can make, but we can do better. Let’s add some elements we learned about earlier. ggplot(data = data_norway, mapping = aes(x = year, y = lifeExp)) + geom_line(size = 2, color = &quot;darkred&quot;) + # size refers to the width of the line labs(x = &quot;Year&quot;, y = &quot;Life expectancy at birth (years)&quot;) + theme_norment() + theme() Now I want a scatter plot for all countries that had data recorded in 2007. At the same time, I’m also interested in the relationship with life expectancy with GDP per capita. So I’ll plot GDP per capita on the x-axis, and life expectancy on the y-axis. I want to color based on the continent, and the {gapminder} package has a set of colors for the continents stored in continent_colors. ggplot(data = data_2007, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(mapping = aes(color = continent), size = 3, alpha = 0.75) + # I want to only apply the color to the points geom_smooth(method = &quot;loess&quot;, color = &quot;grey20&quot;) + # add a loess model to the points scale_color_manual(values = continent_colors) + labs(x = &quot;GDP per capita (US$)&quot;, y = &quot;Life expectancy at birth (years)&quot;, color = NULL) + theme_norment() + theme() If we really wanted to recreate the famous Gapminder plots, we should make the size of the points dependent on the size of the populations, which we can do. ggplot(data = data_2007, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(mapping = aes(color = continent, size = pop), alpha = 0.75) + # I want to only apply the color to the points geom_smooth(method = &quot;loess&quot;, color = &quot;grey20&quot;) + # add a loess model to the points scale_size_continuous(range = c(0.5,10)) + scale_color_manual(values = continent_colors) + labs(x = &quot;GDP per capita (US$)&quot;, y = &quot;Life expectancy at birth (years)&quot;, color = &quot;Continent&quot;, size = &quot;Population&quot;) + theme_norment() + theme( legend.box = &quot;vertical&quot; ) 10.8 Repeated-Meausures ANOVA Okay, so now on to the more complicated work. Here I want to do a repeated-measures ANOVA. It is basically the same as running a regular ANOVA, instead now we want to include time as an interaction effect, and how else to add an interaction effect than to multiply the predictor by the effect of time. A + in this context, means a covariate, which should be interpreted as “controlled for ”. Performing a post-hoc in this context makes little sense, since we don’t want to run more pairwise T-tests for more than 100 countries. rep_anova_model &lt;- aov(lifeExp ~ country*year + gdpPercap, data = gapminder) anova(rep_anova_model) ## Analysis of Variance Table ## ## Response: lifeExp ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## country 141 210176 1491 313.695 &lt; 2.2e-16 *** ## year 1 53919 53919 11347.140 &lt; 2.2e-16 *** ## gdpPercap 1 237 237 49.859 2.578e-12 *** ## country:year 141 13074 93 19.513 &lt; 2.2e-16 *** ## Residuals 1419 6743 5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["a-full-analysis-script.html", "11 A Full Analysis Script", " 11 A Full Analysis Script Okay, so all these code snippets are fun, but what would a full script look like? Here I’ll go through how I would typically write an analysis script. I’m sure those from others might look different, because they prefer to do things a little differently, but here’s mine. I’ll use some simulated data from TOP, so this is not real data. I’ll use a .csv-file here. If your data is currently in SPSS format, the easiest way to load this into R is by converting the SPSS file to a csv-file within SPSS. This is relatively simple: Open your dataset in SPSS Under the File tab, click Save as... Under Save as type:, select Comma delimited (*.csv) Also toggle the Save value labels where defined instead of data values Make sure Write variable names to file is selected Let’s now set up the packages that we want to use, and let’s load in the data with the read.table() function. The header option specifies whether the first line of the file should be considered a row with the column names, or variable names. The sep option specifies the delimiter for the file. If you exported from SPSS like described above, then the delimiter is a semicolon. library(tidyverse) library(ggpubr) library(normentR) data &lt;- read.csv2(&quot;TOPdemographics.csv&quot;) Then it’s generally best practice to inspect the data, so we can see what variables we have available with the names() function. If you’re working in Rstudio (which I recommend), then you can also just click on the data variable, or type View(data) to get a panel where the data is laid out like a spreadsheet, as you would get in SPSS. names(data) ## [1] &quot;nnid&quot; &quot;Diag_category&quot; &quot;Gender&quot; &quot;Pasient_Kontroll&quot; ## [5] &quot;Ethnicity&quot; &quot;AgeBaseline&quot; &quot;score_panss&quot; &quot;score_g&quot; ## [9] &quot;score_n&quot; &quot;score_p&quot; &quot;YMRS_score&quot; Next, let’s investigate, how many patients and how many controls do we have. The simplest way to do this is to create a table, using the table() function, we input to this the Pasient_Kontroll column. table(data$Pasient_Kontroll) ## ## Control Patient ## 1221 1689 Next, we also want to know what diagnoses our patients have, so we’ll do the same for the Diag_category column. table(data$Diag_category) ## ## bipolar I bipolar II ## 1248 305 183 ## bipolarNOS cyclothym major depressive disorder ## 30 2 68 ## other psychosis schizoaffective schizophrenia ## 218 136 667 ## schizophreniform ## 53 So now let’s say we want to compare schizophrenia spectrum with bipolar disorder spectrum patients. We then add to that column the diagoses, where we group the schizophrenia, schizophreniform, and schizoaffective patients together in one schizophrenia-spectrum group, and do the same for the bipolar patients. We can do this with the fct_collapse() function. We store the result in a new column in the data data frame, called Diag_collapsed. This function is very similar to the recode function in SPSS. There’s a little catch though. If you type levels(data$Diag_category). you get the different options that this variable can have. It’s similar to looking at the “values” column in SPSS’ variable tab. Here we can see that one of the levels is just \" \", which is different from NA since it’s just a character string with one empty space. These indicate individuals that have no diagnosis (yet). If we could just recode all of these into \"HC\", that’d be great, but this column also involved patients that have no confirmed diagnosis yet. That’s why we also need a second option. Before we can recode, first we need to change the value in the Diag_collapsed column from subjects where the Pasient_Kontroll column indicates that it is a patient to \"Other\", which is likely the most appropriate value for now. The easiest way to do this was via indexing. An example: print(data$Diag_category) will give the same result as print(data[ ,\"Diag_category\"]). In indexing, the first position is for the row number, the second for the column name or number. Just for illustration, typing data[1000,\"nnid\"], will print the ID from the 1000th participant in the database. This is because indexing is done like this: data[&lt;row&gt;,&lt;column&gt;], if either the &lt;row&gt; or &lt;column&gt; in this example is left empty, then it will take all rows or column, for instance data[1000, ] will print all columns from the 1000th participant, while data[ ,\"Diag_category\"] will print the Diag_category column from all participants. Below, we’ll use it to select all rows where the Pasient_Kontroll column is equal to \"Patient\", and then apply the value \"Other\" only to the \"Diag_collapsed\" column. Afterwards, we can run the fct_collapse() function I discussed earlier. levels(data$Diag_category) ## [1] &quot; &quot; &quot;bipolar I&quot; &quot;bipolar II&quot; ## [4] &quot;bipolarNOS&quot; &quot;cyclothym&quot; &quot;major depressive disorder&quot; ## [7] &quot;other psychosis&quot; &quot;schizoaffective&quot; &quot;schizophrenia&quot; ## [10] &quot;schizophreniform&quot; data[data$Pasient_Kontroll == &quot;Patient&quot; &amp; data$Diag_category == &quot; &quot;, &quot;Diag_collapsed&quot;] = &quot;Other&quot; data$Diag_collapsed &lt;- fct_collapse(data$Diag_category, SCZ_spect = c(&quot;schizophrenia&quot;, &quot;schizophreniform&quot;, &quot;schizoaffective&quot;), BD_spect = c(&quot;bipolar I&quot;, &quot;bipolarNOS&quot;, &quot;bipolar II&quot;), HC = &quot; &quot;, group_other = TRUE) Now we have a column called Diag_collapsed that has four levels: \"SCZ_spect\", \"BD_spect\", \"Other\", and \"HC\". So what are the numbers now? table(data$Diag_collapsed) ## ## HC BD_spect SCZ_spect Other ## 1248 518 856 288 Suppose we’re interested in YMRS and PANSS scores. Let’s immediately dive into some plotting. ggplot(data = data, mapping = aes(x = Diag_collapsed, y = score_panss)) + geom_violin() + theme_norment() ## Warning: Removed 1286 rows containing non-finite values (stat_ydensity). ggplot(data = data, mapping = aes(x = Diag_collapsed, y = YMRS_score)) + geom_violin() + theme_norment() ## Warning: Removed 1445 rows containing non-finite values (stat_ydensity). From these plots we can see that the data is quite skewed, especially the YMRS. So before we want do any statistics, we should probably quantify whether the data is normally distributed. For this we have the shapiro.test() that we’ve used before. shapiro.test(data$score_panss) ## ## Shapiro-Wilk normality test ## ## data: data$score_panss ## W = 0.94831, p-value &lt; 2.2e-16 shapiro.test(data$YMRS_score) ## ## Shapiro-Wilk normality test ## ## data: data$YMRS_score ## W = 0.80452, p-value &lt; 2.2e-16 So it appears that both variables are significantly different from a normal distribution, so we violate some assumptions of the statistical tests we might do, such as the T-test and the ANOVA and in general all parametric tests. So we need to consider a non-parametric test. Since we’re interested only in the comparison between schizophrenia, bipolar disorder, and healthy controls, we remove all incidences of \"Other\" from the dataset, we do this by using the filter() function. We’ve used the == operator before, but now we’ll use the opposite !=, which translates to “not equal to”. Then we’ll use the non-parametric equivalent of the ANOVA, which is the Kruskal-Willis test, implemented in R with the kruskal.test() function. kruskal.test(formula = score_panss ~ Diag_collapsed, data = data) ## ## Kruskal-Wallis rank sum test ## ## data: score_panss by Diag_collapsed ## Kruskal-Wallis chi-squared = 432.71, df = 3, p-value &lt; 2.2e-16 Then we’ll want to make some plot, preferably a pretty boxplot. Note that since healthy controls don’t have a score on either the PANSS or the YMRS, this will be empty, and R will tell you that it couldn’t show a number of rows. data_plot &lt;- filter(data, Diag_collapsed != &quot;HC&quot;) bplot &lt;- ggplot(data_plot, aes(x = Diag_collapsed, y = score_panss, fill = Diag_collapsed)) + geom_boxplot(width = 0.5, alpha = 0.5) + stat_compare_means(method = &quot;kruskal&quot;) + labs(x = &quot;Diagnosis&quot;, y = &quot;PANSS score (total)&quot;, fill = NULL) + scale_x_discrete(labels = c(&quot;BD&quot;, &quot;SCZ&quot;,&quot;Other&quot;)) + scale_fill_norment(discrete = TRUE, palette = &quot;powerpoint&quot;, labels = c(&quot;SCZ&quot;, &quot;BD&quot;, &quot;HC&quot;)) + theme_norment() + theme() print(bplot) ggsave(plot = bplot, file = &quot;figures/boxplot.png&quot;) ## Saving 7 x 5 in image The ggsave() function saves the plot as an image in the working directory specified. If you don’t specify a folder, it will save it in the current working directory, you can get the current working directory by typing getwd(). Let’s also run a correlation, between the age and the score on the PANSS for patients with schizophrenia only. We can do this with the cor.test() function. You input the two columns you want to use, and the most appropriate method (either \"pearson\", \"kendall\", or \"spearman\"). data_scz &lt;- filter(data, Diag_collapsed == &quot;SCZ_spect&quot;) cor.test(data_scz$AgeBaseline, data_scz$score_panss, method = &quot;spearman&quot;) ## Warning in cor.test.default(data_scz$AgeBaseline, data_scz$score_panss, : Cannot compute exact p- ## value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: data_scz$AgeBaseline and data_scz$score_panss ## S = 109823369, p-value = 0.0002118 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.1277889 Let’s also make a scatter plot to see if this makes sense. scplot &lt;- ggplot(data_scz, aes(x = AgeBaseline, y = score_panss, color = Gender)) + geom_point(alpha = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;grey30&quot;) + labs(x = &quot;Age at baseline&quot;, y = &quot;Score PANSS (total)&quot;, color = &quot;Sex&quot;) + scale_color_norment(discrete = TRUE, palette = &quot;logo&quot;) + theme_norment() + theme() print(scplot) ggsave(scplot, file = &quot;figures/scatterplot.png&quot;) ## Saving 7 x 5 in image Yes, it does, it seems there’s no direct association between age and PANSS, I’ll just say that I’m not a psychologist, and I’m not sure whether we should expect a correlation and what important covariates I missed, but this is just for illustration purposes. All in all, this is what a simple script would look like. In the full script below I’ve added comments to make things clearer. If you hadn’t noticed it yet, comments are added by typing #, followed by whatever your comment is. Comments are not evaluated when running the code. # Load packages #### library(tidyverse) library(ggpubr) library(normentR) # Load data #### data &lt;- read.csv2(&quot;TOPdemographics.csv&quot;) # Inspect data #### names(data) table(data$Pasient_Kontroll) table(data$Diag_category) # Collapse diagnoses #### levels(data$Diag_category) data[data$Pasient_Kontroll == &quot;Patient&quot; &amp; data$Diag_category == &quot; &quot;, &quot;Diag_collapsed&quot;] = &quot;Other&quot; data$Diag_collapsed &lt;- fct_collapse(data$Diag_category, SCZ_spect = c(&quot;schizophrenia&quot;, &quot;schizophreniform&quot;, &quot;schizoaffective&quot;), BD_spect = c(&quot;bipolar I&quot;, &quot;bipolarNOS&quot;, &quot;bipolar II&quot;), HC = &quot; &quot;, group_other = TRUE) table(data$Diag_collapsed) # Inspect clinical variables #### ggplot(data = data, mapping = aes(x = Diag_collapsed, y = score_panss)) + geom_violin() + theme_norment() ggplot(data = data, mapping = aes(x = Diag_collapsed, y = YMRS_score)) + geom_violin() + theme_norment() # Run statistics #### # Test for normality shapiro.test(data$score_panss) shapiro.test(data$YMRS_score) # Kruskal-Wallis data_clean &lt;- filter(data, Diag_collapsed != &quot;Other&quot;) kruskal.test(formula = score_panss ~ Diag_collapsed, data = data_clean) data_plot &lt;- filter(data_clean, Diag_collapsed != &quot;HC&quot;) # Create boxplot bplot &lt;- ggplot(data_plot, aes(x = Diag_collapsed, y = score_panss, fill = Diag_collapsed)) + geom_boxplot(width = 0.5, alpha = 0.5) + stat_compare_means(method = &quot;kruskal&quot;) + labs(x = &quot;Diagnosis&quot;, y = &quot;PANSS score (total)&quot;, fill = NULL) + scale_x_discrete(labels = c(&quot;BD&quot;, &quot;SCZ&quot;)) + scale_fill_norment(discrete = TRUE, palette = &quot;powerpoint&quot;, labels = c(&quot;BD&quot;, &quot;SCZ&quot;)) + theme_norment() + theme() print(bplot) ggsave(plot = bplot, file = &quot;boxplot.png&quot;) # Correlation between age and PANSS data_scz &lt;- filter(data_clean, Diag_collapsed == &quot;SCZ_spect&quot;) cor.test(data_scz$AgeBaseline, data_scz$score_panss, method = &quot;spearman&quot;) scplot &lt;- ggplot(data_scz, aes(x = AgeBaseline, y = score_panss, color = Gender)) + geom_point(alpha = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;grey30&quot;) + labs(x = &quot;Age at baseline&quot;, y = &quot;Score PANSS (total)&quot;, color = &quot;Sex&quot;) + scale_color_norment(discrete = TRUE, palette = &quot;logo&quot;) + theme_norment() + theme() print(scplot) ggsave(scplot, file = &quot;scatterplot.png&quot;) "],
["futher-reading.html", "12 Futher Reading", " 12 Futher Reading If you’re looking for a place to learn more about R, then I’d recommend the GardenersOwn.co.uk website, which has some tutorials on basic statistics in R. There’s an almost infinite amount of questions and answers on various R topics on StackOverflow.com. In addition I’d also really recommend the following accounts on Twitter: R4DataScience, which is an account with the explicit purpose to make R more accessible. Related to this is also Thomas Mock, who organizes a weekly exercise, called TidyTuesday, which is a initiative where they share a public dataset, and invite all interested Twitter users to share their statistics and particularly their data visualizations. I’d also recommend following Hadley Wickham and Thomas Lin Pedersen. Both work for Rstudio, Hadley Wickham is the Chief Scientist and brain behind the {ggplot2} package. Both Hadley and Thomas work on the {tidyverse} package. Thomas is also interesting to follow for his genetive artwork, which he creates in part by using R. I’d also recommend checking the Twitter profile from our very own Dan Quintana, he posts little videos occasionally where he live codes to solve a certain problem. In that same category, I’d also recommend searching YouTube for tutorials. In particular live coding videos are useful since you can follow along in their thought and coding process. "],
["appendix-semi-advanced-r.html", "13 Appendix: semi-advanced R 13.1 Sharing your data 13.2 For-loops 13.3 The pipe 13.4 Working with dates 13.5 Animations 13.6 Cowplot", " 13 Appendix: semi-advanced R Now, if all the stuff that I discussed earlier was too simple for you already, then you may be interested in the next section. This will go over some more advanced methods and tools within R. If the previous sections were already a challenge, then I’d actually urge you to skip this step and get a good hold of the principles discussed above before moving on to this section as to avoid freaking you out with complicated looking code. 13.1 Sharing your data So I discussed earlier on how to easily move your data from SPSS to R, but what if you want to share your dataset with a colleague or make it publicly available so others can reproduce your findings? There’s a few ways of going about this. I’m assuming you are finished with your analysis, and you have one big data frame with a bunch of variables merged, other data extracted, and it has undergone a cleaning process. If you want to share just one data frame, the easiest way is to write it a tab-delimited csv-file. The function you would use here is the write.table() function (note the similarity to the read.table() function). This function takes a number of inputs, but I’ll typically use only three of the optional settings, which are the quote, sep, and row.names option. The quote = FALSE option tells R that you don’t want to save strings with the double quotes. This is useful since not all programs treat those double quotes the same way. In the second option, sep, you tell R what you want the delimiter to be. A common delimiter is the semicolon (\";\"), but the most universally accepted delimiter is the tab. That means that each column is separated by a tab, which is different from just a white space sometimes. We set the delimiter to a tab by setting the sep option as follows: \"\\t\". The last option I always use is the row.names option, which I set to FALSE. This tells R that I don’t want to save the row names (which is just a sequence from 1 to how many rows your data frame has, i.e. the row numbers). It may be confusing and if you did your job properly, you’d have an ID column in there anyway, making the row numbers redundant. Since it’s usually just confusing and annoying, I get rid of it. write.table(data_comb, file = &quot;FileToShare.csv&quot;, quote = FALSE, sep = &quot;\\t&quot;, row.names = FALSE) Another way of sharing your data is by creating an Rdata file, this is an R specific format, which is not compatible with other programs usually. The biggest advantage of the Rdata file is that you can save all variables you have created in one file. And the next person who loads this Rdata file will have the exact same workspace you used, so there is little confusion on how your workspace was organized. You can also save a single data frame in an Rdata file, but if you are thinking about doing that, then I’d recommend to use the write.table() function instead. We can save all variables in an Rdata file using the save.image() function: save.image(file = &quot;AllVariables.Rdata&quot;) This is really simple. If you wanted to save several variables, you can use the save() function: save(list = c(&quot;data&quot;, &quot;data_clean&quot;, &quot;bplot&quot;), file = &quot;SomeVariables.Rdata&quot;) Then you can send the files you’ve created to your collaborators, who can read the csv-files using the regular read.table() function. The Rdata files can be loaded by using the load() function as follows: load(&quot;AllVariables.Rdata&quot;) # or load(&quot;SomeVariables.Rdata&quot;) 13.2 For-loops Now, for those of you that are left, hello! In this section we’ll go over loops, the pipe, and long vs. wide formats. We’ll start with the simplest, the “for”-loop. The for loop I can explain in a simple way. In the example below, we loop over a variable called vector, which is also a vector from one to ten. The principle of a for loop is that the code within the loop remains the same, but one variable changes. The changing variable is the variable that is “looped over”. I the example, the loop has 10 iterations, and in the first iteration the variable i, has the value 1, in the second the value 2, and in the tenth the value 10. The context in which we’ve used it now, in the first iteration, the loop will print a vector from i to 11. As i increases, the list will become shorter and shorter. Note how the first number in each iteration is always equal to i, and the last number always 11. vector &lt;- 1:10 for (i in vector) { print(i:11) } ## [1] 1 2 3 4 5 6 7 8 9 10 11 ## [1] 2 3 4 5 6 7 8 9 10 11 ## [1] 3 4 5 6 7 8 9 10 11 ## [1] 4 5 6 7 8 9 10 11 ## [1] 5 6 7 8 9 10 11 ## [1] 6 7 8 9 10 11 ## [1] 7 8 9 10 11 ## [1] 8 9 10 11 ## [1] 9 10 11 ## [1] 10 11 The for loop is particularly useful for situations where one needs to run the same code multiple times. I use it often in situations where I have multiple files that I want to combine in one data frame, or when I want to apply the same operation on multiple variables with similar names (e.g. \"edge1\" to \"edge210\"). The for loop is very useful, but it’s relatively slow and sometimes difficult to read, but it is definitely better than any lapply() function or similar, since that’s ugly code. I am aware that some people on online help forums like lapply() and recommend it quickly, but I think writing a for loop is definitely preferred. 13.3 The pipe The pipe (%&gt;%) was probably the most useful function I have learned to use. The principle of the pipe is that it takes whatever the previous data or function output was and it puts it into the next function. Let’s try it with the filter() function, we want to select only the healthy controls in the next dataset, and I’ll show two ways of using the same function with and without the pipe. Then we’ll check if both data frames are identical with the all.equal() function. data_HC1 &lt;- filter(data, Pasient_Kontroll == &quot;Control&quot;) data_HC2 &lt;- data %&gt;% filter(Pasient_Kontroll == &quot;Control&quot;) all.equal(data_HC1, data_HC2) ## [1] TRUE So we see that both data frames are identical. I find that the pipe is useful, especially when you use the pipe more than once. Let’s try this example again. First we’ll select only the schizophrenia-spectrum patients only, then select only participants in the age range 20 to 30, and then we want to summarise the total PANSS scores for each diagnosis category within this spectrum. We calculate the mean and standard deviation of the PANSS score for each diagnosis. The group_by() function is very useful! You can add the group_by() function in almost any situation, and it will apply the functions further down the pipe by group instead of for the whole dataset. I use it a lot. If you want to remove the grouping, you can use ungroup() at the end. data_p &lt;- data %&gt;% filter(Diag_collapsed == &quot;SCZ_spect&quot;, AgeBaseline &gt;= 20 &amp; AgeBaseline &lt;= 30) %&gt;% group_by(Diag_category) %&gt;% summarise(mean_panss = mean(score_panss, na.rm = TRUE), sd_panss = sd(score_panss, na.rm = TRUE), n = n()) print(data_p) ## # A tibble: 3 x 4 ## Diag_category mean_panss sd_panss n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 schizoaffective 57.3 12.7 67 ## 2 schizophrenia 68.2 18.4 347 ## 3 schizophreniform 54 14.8 30 13.4 Working with dates At some point in your analysis, you might have to work with dates, for instance to calculate the difference between two dates to obtain the age. Working with dates is a messy job in any software, and R is no exception. Let’s look at some particularly messy data. We have a dataset of 5 individuals, we have their date of birth, the date of the first interview, and the date of the second interview. We are interested in two things, the age at the first interview, and the time between the first and second interview. Let’s have a look at our little dataset. print(data) ## names DoB interview1 interview2 ## 1 Lucie 03/11/&#39;78 03.05.2015 20170607 ## 2 Ane 20/12/&#39;88 30.08.2015 20171004 ## 3 Andreas 15/05/&#39;77 28.03.2016 20180412 ## 4 Kyrre 18/06/&#39;90 15.12.2016 20190201 ## 5 Vilde 29/01/&#39;86 30.11.2017 20190108 Looks pretty straightforward right? Let’s also have a look at the column types: str(data) ## &#39;data.frame&#39;: 5 obs. of 4 variables: ## $ names : Factor w/ 5 levels &quot;Andreas&quot;,&quot;Ane&quot;,..: 4 2 1 3 5 ## $ DoB : Factor w/ 5 levels &quot;03/11/&#39;78&quot;,&quot;15/05/&#39;77&quot;,..: 1 4 2 3 5 ## $ interview1: Factor w/ 5 levels &quot;03.05.2015&quot;,&quot;15.12.2016&quot;,..: 1 4 3 2 5 ## $ interview2: num 20170607 20171004 20180412 20190201 20190108 So we see that we have a number of character columns, which are interpreted as factors, and one numeric one. In order to work with dates appropriately, we need to convert all of these into a standard format. There’s a simple function for this: as.Date(). This function converts a character column into the standardized date format. Note that it only takes a character column as input, and no numeric or integer ones. We’ll get back to that issue later. The as.Date() function takes another input, the format in which it should interpret the character string. There’s a couple of items here, for instance %d stands for “day”, %m stands for “month”, %y stands for a two-digit year (i.e. ’82), %Y stands for a four-digit year (i.e. 1982). If you had a character string that looked like this: \"17/02/1992\", then the format you’d put in the as.Date() function would look like this: \"%d/%m/%Y\", and \"01.26.65\" would be \"%m.%d.%y\". Let’s tidy up our data first, and then we’ll get to the calculations later. We’ll use the pipe we described before, as well as the mutate() function, we’ll do all three conversions at the same time, hopefully so you can see the pattern. Since the format of the interview2 column is a numeric one, we’ll have to convert that to a character format first, we’ll do that inside the as.Date() function by using the as.character() function. data &lt;- data %&gt;% mutate(DoB = as.Date(DoB, format = &quot;%d/%m/&#39;%y&quot;), interview1 = as.Date(interview1, format = &quot;%d.%m.%Y&quot;), interview2 = as.Date(as.character(interview2), format = &quot;%Y%m%d&quot;)) Let’s now have a look at the data again: print(data) ## names DoB interview1 interview2 ## 1 Lucie 1978-11-03 2015-05-03 2017-06-07 ## 2 Ane 1988-12-20 2015-08-30 2017-10-04 ## 3 Andreas 1977-05-15 2016-03-28 2018-04-12 ## 4 Kyrre 1990-06-18 2016-12-15 2019-02-01 ## 5 Vilde 1986-01-29 2017-11-30 2019-01-08 We can see that the date columns are now converted to a standard date format, this format looks like this \"%Y-%m-%d\". Now all our date columns are in a unified standardized format. Let’s now do some calculations, for this we’ll use the {lubridate} package. We want to calculate age at the first interview, and the time between the first and second interview. Let’s calculate age first. We’ll use the difftime() function to calculate the difference between two timepoints, in our date, the date of birth, and the time of the first interview. age_difftime &lt;- difftime(data$interview1, data$DoB) print(age_difftime) ## Time differences in days ## [1] 13330 9749 14197 9677 11628 We see that it gives us the difference in days. We’re not particularly interested in how old people are in days, we’re particularly interested in years. However, dividing by 365.25 is not entirely accurate, and many other manual methods will undoubtedly give us a massive headache. Especially when we have to account for the year 2000, which should not have been a leap year, but was. Luckily there’s a package that handles this headache for us, the {lubridate} package (not to be misread as “lubricate”). This package contains one particularly useful function, called time_length(). This function takes the time difference in days and converts it to whatever you specify you want to get back (specified in the unit option). Let’s do it: library(lubridate) time_length(age_difftime, unit = &quot;years&quot;) ## [1] 36.52055 26.70959 38.89589 26.51233 31.85753 We now have a number of ages for our participants, with decimals. If we don’t want that, we can round it: round(time_length(age_difftime, unit = &quot;years&quot;)) ## [1] 37 27 39 27 32 If we wanted to add this as a column to our data frame, we could combine the steps above into one. It would look something like this: data &lt;- data %&gt;% mutate(age_intv1 = round(time_length(difftime(interview1, DoB), unit = &quot;years&quot;))) print(data) ## names DoB interview1 interview2 age_intv1 ## 1 Lucie 1978-11-03 2015-05-03 2017-06-07 37 ## 2 Ane 1988-12-20 2015-08-30 2017-10-04 27 ## 3 Andreas 1977-05-15 2016-03-28 2018-04-12 39 ## 4 Kyrre 1990-06-18 2016-12-15 2019-02-01 27 ## 5 Vilde 1986-01-29 2017-11-30 2019-01-08 32 Let’s say you wanted to know the difference between the first and second interview in weeks with decimals (I don’t know why anyone would prefer to know it in weeks rather than days or years, but I won’t judge). We can do that! Just for fun, we’ll also calculate the age of the participant at the second interview at the same time, data &lt;- data %&gt;% mutate(interval = time_length(difftime(interview2, interview1), unit = &quot;weeks&quot;), age_intv2 = round(time_length(difftime(interview2, DoB), unit = &quot;years&quot;))) print(data) ## names DoB interview1 interview2 age_intv1 interval age_intv2 ## 1 Lucie 1978-11-03 2015-05-03 2017-06-07 37 109.42857 39 ## 2 Ane 1988-12-20 2015-08-30 2017-10-04 27 109.42857 29 ## 3 Andreas 1977-05-15 2016-03-28 2018-04-12 39 106.42857 41 ## 4 Kyrre 1990-06-18 2016-12-15 2019-02-01 27 111.14286 29 ## 5 Vilde 1986-01-29 2017-11-30 2019-01-08 32 57.71429 33 And now you have the age of the partipant at the two interview, and the difference (in weeks) between the first and second interview. The syntax for these calculations isn’t particularly complicated, but it takes some cognitive effort to recognize the patterns. Anyway, other than punching the age of the participant when collecting the data, this is the easiest and cleanest way I could think of for dealing with dates in R. 13.5 Animations Remember that animation I used in the “distributions” section, here’s how I made it. Creating animations from ggplot objects is pretty simple with the {gganimate} package. First I created a perfect distribution, and repeated it 10 times, then I created 10 different random distributions, and then repeated the first one as the last one. This last step was necessary to make sure the transitions would be smooth throughout the animation. Then I created the ggplot object, and added the transition_time() function to tell the {gganimate} package what variable to create the animation over. Then I also added the ease_aes() layer to create smooth transitions with tiny pauses at each step. library(gganimate) n_replications &lt;- 100 norm &lt;- data.frame() temp &lt;- data.frame() for (i in 1:11) { temp &lt;- data.frame( sum_norm = seq(2,12), prob_norm = c(1/36, 2/36, 3/36, 4/36, 5/36, 6/36, 5/36, 4/36, 3/36, 2/36, 1/36) * n_replications, it = i ) norm &lt;- rbind(norm,temp) } outcomes_anim &lt;- data.frame() temp &lt;- data.frame(x = 1:n_replications) for (i in 1:10) { temp$it &lt;- rep(i,n_replications) temp$sum_rand &lt;- replicate(n = n_replications, expr = sum(sample(1:6, 2, replace = TRUE))) outcomes_anim &lt;- rbind(outcomes_anim,temp) } outcomes_anim &lt;- rbind(outcomes_anim, outcomes_anim %&gt;% filter(it == 1) %&gt;% mutate(it = 11)) %&gt;% mutate(it = as.integer(it)) ggplot() + geom_col(data = norm, mapping = aes(x = sum_norm, y = prob_norm), color = &quot;black&quot;, fill = &quot;black&quot;, alpha = 0.5) + geom_histogram(data = outcomes_anim, mapping = aes(x = sum_rand), color = norment_colors[[&quot;magenta&quot;]], fill = norment_colors[[&quot;magenta&quot;]], alpha = 0.5, binwidth = 1) + scale_x_continuous(breaks = seq(2,12)) + labs(x = &quot;Outcome&quot;, y = &quot;Frequency&quot;) + theme_norment() + theme( panel.grid.minor.x = element_blank() ) + transition_time(it) + ease_aes(&#39;cubic-in-out&#39;) ## Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) ## Finalizing encoding... done! 13.6 Cowplot Now I just wanted to give introduction to the {cowplot} package. It has a number of useful functions for plotting with ggplot (some of which are introducted here). I mainly wanted to give introduction to a method by which one could show distributions in the same figure as a regular scatter plot. Let’s grab back to the scatter plot we made in the main text: Now, we can insert density plots along the x- and y-axis. We can do that with the axis_canvas() function, which we can apply to both the axes: xdens &lt;- axis_canvas(scplot, axis = &quot;x&quot;) + geom_density(data = data_scz, aes(x = AgeBaseline, fill = Gender), alpha = 0.5, size = 0.2) + scale_fill_norment(discrete = TRUE, palette = &quot;logo&quot;) Now we have an object which we’ll later insert along the x-axis. For the y-axis we’ll do the same, but since we want to flip this density plot by 90 degrees, we need to set coord_flip to true and add coord_flip() after the geom_density() function. ydens &lt;- axis_canvas(scplot, axis = &quot;y&quot;, coord_flip = TRUE) + geom_density(data = data_scz, aes(x = score_panss, fill = Gender), alpha = 0.5, size = 0.2) + scale_fill_norment(discrete = TRUE, palette = &quot;logo&quot;) + coord_flip() Next, we insert both these objects into their respective axes in the plot. We can do this with the insert_xaxis_grob() and insert_yaxis_grob(). It needs some other trickery to position it correctly and we’ll need to do it first for one axis and then for another, but it’s relatively simple: scplot_xdens &lt;- insert_xaxis_grob(scplot, xdens, grid::unit(0.2, &quot;null&quot;), position = &quot;top&quot;) scplot_xydens &lt;- insert_yaxis_grob(scplot_xdens, ydens, grid::unit(0.2, &quot;null&quot;), position = &quot;right&quot;) Then we can draw the plot. Since we’ve used a combination of functions from different packages to get the plot object we now have, we can no longer plot the figure as we did before. Instead we need to use the ggdraw() function. ggdraw(scplot_xydens) And now we have density plots on the axes showing the distribution of the values for both variables! You could do the same with for instance geom_histogram()! Happy plotting! "],
["appendix-semi-advanced-statistics.html", "14 Appendix: semi-advanced statistics 14.1 ANOVA 14.2 Everything is a linear model", " 14 Appendix: semi-advanced statistics Here I’ll go more in-depth into some concepts I think are important. I’ll probably dive deeper into the process underneath some statistical tests, in particular ANOVA, since it’s so often done misunderstood. 14.1 ANOVA Here I want to go into some of the intricacies of the repeated-measures ANOVA I’ll go through some of the assumptions, and describe the operations underlying the ANOVA. This section is based on an online lecture and tutorial by Andrew Conway. We’ll first calculate every manually, (i.e. sum of squares, mean squares etc.) so that we get the F statistic and the p-value. At the end of this long paragraph, I’ll show how to get the same results in one or two lines of code. But of course the purpose of this paragraph is to lay out some of the underlying statistics beneath ANOVA, as to understand better what the output of any ANOVA is were we to run it like we do commonly. Let’s say we ran a longitudunal study, where we train a group of people to perform a difficult task, which is scored from 0 to 10. The data is stored in a variable called data. Let’s say we tested 20 individuals, and we investigated how their test scores improved after 8, 12, 17, and 19 days of training. Here’s the summary of the data that we have: summary(data) ## id condition score ## 1 : 4 8 days :20 Min. :2.700 ## 2 : 4 12 days:20 1st Qu.:5.487 ## 3 : 4 17 days:20 Median :6.325 ## 4 : 4 19 days:20 Mean :6.408 ## 5 : 4 3rd Qu.:7.562 ## 6 : 4 Max. :9.600 ## (Other):56 One of the assumptions of a repeated-measures ANOVA is the assumption of sphericity. Sphericity in this context means that the amount of variance of the differences (i.e difference in test scores) between all possible comparisons of within-subject conditions (i.e. days of training) is equal. We use the Mauchly’s Test to test this assumption. This test is implemented in R as the mauchly.test() fuction. In order to test this assumption, we need a little matrix with the participants along the rows, and the test scores as values inside separate columns. In our case we’ll have a 20 x 4 matrix. Let’s create that with the pivot_wider() function. This takes the condition levels (i.e. the new column names) and adds every level as a column. The score (aka value) is entered as values in these columns. As default, it also carries over the id column, which we dont want, so we deselect it with the select() function, and then we convert it from a data frame to a matrix using the as.matrix() function. score_matrix &lt;- data %&gt;% pivot_wider(names_from = condition, values_from = score) %&gt;% select(-&quot;id&quot;) %&gt;% as.matrix() Then, since the mauchly.test() function requires a linear model as input, we first need to create that. We take the matrix with the scores, and add it into an lm() function. Since we don’t want to run it against anything else, we run the lm by 1 (which incidentally is equivalent to running a T-test, but that’s a story for another time). Then we add the linear model into the mauchly.test() function, and specify that we ran it against 1 in the x option. lm_model &lt;- lm(score_matrix ~ 1) mauchly.test(lm_model, x = ~ 1) ## ## Mauchly&#39;s test of sphericity ## ## data: SSD matrix from lm(formula = score_matrix ~ 1) ## W = 0.81725, p-value = 0.9407 If the Mauchly test is significant, then we know that our data is significantly different from the assumption of sphericity, we we want a non-signficicant result on this test. In this case, the p-value is 0.9407, which means that we can assume that we meet the assumption of sphericity, and can proceed. If this test would come up significant, then we’d have to add a correction to the futher analysis, typically a Greenhouse-Geisser correction or a Huyn-Feldt correction. The Greenhouse-Geisser correction is incorporated in the {afex} package. From here, running a repeated measures is quite simple with the functions available in R, but the purpose of this paragraph is to do it manually, so below we’ll calculate the F-ratio ourselves. The first thing we need for this is the variance of the grouping condition, or the between-groups variance. We calculate the sum of squares, which is the number of participants multiplied by the sum of the group mean minus the overall mean, squared. This is easier explained in a formula where yj (j stands for the number of levels) is the group mean and the other y is the overall mean across conditions and n is the number of pariticpants. \\[ss_{a} = n\\sum (y_{j} - \\overline{y})^2\\] We can translate this formula to R as follows. In summary, first we determine the number of participants (which we know, but it’s good practice to make it more dynamic). Then we calculate the group means by grouping the data variable and calulating the mean score for each level of the condition. This will give us a data frame with the condition as one column, and the mean score in another column called y_j. Then we calculate the sum of squares by translating the formula above into R code. n &lt;- length(unique(data$id)) # number of participants in dataset group_means &lt;- data %&gt;% group_by(condition) %&gt;% # group by condition summarise(y_j = mean(score)) # calculate group means y_mean &lt;- mean(group_means$y_j) # calculate mean of group means ss_cond &lt;- n * sum((group_means$y_j - y_mean)^2) The sum of squares now represents the systematic between-groups variance. Then we can calculate the mean squares, which represents systematic variance. The formula for this is simply the sum of squares we calculated before divided by the degrees of freedom of the condition, which is the number of levels inside the condition minus one. The formula looks like this: \\[ms_{a} = \\frac{ss_{a}}{df_{a}}\\] We already calculated the sum of squares, the degree of freedoms is one less than the number of levels inside the condition, so the formula for this in R is simple: df_cond &lt;- length(unique(data$condition)) - 1 ms_cond &lt;- ss_cond/df_cond The ms_a variable now contains the value that represents the systematic variance based on the condition. Now, since we want to do a repeated-measures ANOVA, we also need to incorporate the error associated with the individual participants that is inevitable when testing them on different time points. So now we’ll do the same we did earlier, but now calculating the sum of squares and mean squares to calculate the variance associated with the participants. We’ll use the same formula, but instead we now use the number of levels in the condition as the n, and the degrees of freedom (df_p) is one less than the number of pariticpants. Notice the differences between the code below and the code above. n &lt;- length(unique(data$condition)) participant_means &lt;- data %&gt;% group_by(id) %&gt;% # group by participant summarise(y_j = mean(score)) # calculate group means y_mean &lt;- mean(participant_means$y_j) # calculate mean of participant means ss_p &lt;- n * sum((participant_means$y_j - y_mean)^2) df_p &lt;- length(unique(data$id)) - 1 ms_p &lt;- ss_p/df_p We now have the systematic variance due to participants, this will serve as the error term later on. Now we also need to think about the unsystematic variance in the data. This is the unsystematic within-group variance. We calculate the mean score by group and then subtract that from the individual scores. data &lt;- data %&gt;% group_by(condition) %&gt;% mutate(meany_i = mean(score)) Then we calculate the within-groups sum of squares and the mean squares like we did before. The degrees of freedom in this case is the number of levels (the exact number, not one less!) multiplied by the number of pariticpants minus one. ss_pa &lt;- sum((data$score - data$meany_i)^2) df &lt;- length(unique(data$condition)) * (length(unique(data$id)) - 1) ms_sa &lt;- ss_pa/df But then we still don’t have the unsystematic variance for the error term, but we do have all the ingredients. In order to calculate this, we simply subtract the systematic variance between participants from the between-groups sum of squares. And then we can use that to calculate the mean squares for this, commonly referred to as the error term. The degrees of freedom in this formula is equal to the number of levels in the condition minus one multiplied by the number of participants minus one. ss_rm &lt;- ss_pa - ss_p df_error &lt;- (length(unique(data$condition)) - 1) * (length(unique(data$id)) - 1) ms_rm &lt;- ss_rm/df_error Now we have all the ingredients to calculate the F-ratio, which is the test statistic of the ANOVA. In order to calculate the effect of the condition on the overall variance, we divide the mean squares of the condition by the mean squares of the mean squares of the error term. In formula form it would look like this: \\[F = \\frac{mean\\ squares\\ of\\ condition\\ (ms_{a})}{Error\\ term\\ (ms_{rm})}\\] The R code for this is simple again. To calculate whether the condition has a significant effect on the test score, we use the F distribution function (pf()), which takes the F ratio, and the degrees of freedom of both the numerator and the denominator. The output from this function is the distribution function, we get the p-value by substracting this distribution function from 1. The code for this looks something like this: F_ratio &lt;- ms_cond/ms_rm p &lt;- 1 - pf(F_ratio, df_cond, df_error) print(p) ## [1] 2.159436e-06 We find that there is a significant effect of the condition on the subject variance! In human language, the length of training affected test scores significantly. This was a lot of hassle with idenitical code coming by several times in perhaps somewhat confusing variable names. Luckily for us, we don’t need to do this every time. In the main text we’ve used the aov() function already. We can use it here to but add the error term in the Error() function and incorporate it into the formula that goes into the aov(). Let’s do all that we’ve done above again, but in two lines: repmeas_model &lt;- aov(score ~ condition + Error(id/condition), data = data) summary(repmeas_model) ## ## Error: id ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 19 43.9 2.311 ## ## Error: id:condition ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 3 49.02 16.341 12.51 2.16e-06 *** ## Residuals 57 74.45 1.306 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you have followed along with the code and looked at the values of each variable we calculated (e.g. ss_p), you might now notice that the last “Residuals” part of the output represents the error term. The values on the condition row is identical to the between-group variance we calculated, and the F-value and p-value are equal to the output we obtained manually! Of course in regular analysis scripts, you would pick the aov() over all the hassle we went through earlier, but the purpose of this paragraph was just to become more familiar with the underlying operations of ANOVA. 14.2 Everything is a linear model Since this requires a rather elaborate description, I’ve made a separate article on this topic. It is still work in progress, but the preliminary version can be found here. "],
["references.html", "References", " References "]
]
